# ReuniÃ£o de Mestrado - 17/10/2025
## Guia de ApresentaÃ§Ã£o: EvoluÃ§Ã£o do Pipeline de Experimentos IDS-IoT

**DuraÃ§Ã£o estimada:** 30 minutos  
**Data:** 17 de outubro de 2025  
**VersÃ£o do Pipeline:** 4.0  

---

## ğŸ“‹ AGENDA DA REUNIÃƒO

1. **Contexto Inicial e LimitaÃ§Ãµes Identificadas** (5 min)
2. **Principais Desafios TÃ©cnicos e SoluÃ§Ãµes** (12 min)
3. **DecisÃµes MetodolÃ³gicas e Embasamento CientÃ­fico** (8 min)
4. **Resultados e Impacto nas ContribuiÃ§Ãµes** (3 min)
5. **PrÃ³ximos Passos** (2 min)

---

## 1. CONTEXTO INICIAL E LIMITAÃ‡Ã•ES IDENTIFICADAS (5 min)

### 1.1 Estado Original do Pipeline

**ConfiguraÃ§Ã£o Inicial:**
- 7 algoritmos de ML (apenas classificadores supervisionados)
- 3-5 configuraÃ§Ãµes de parÃ¢metros por algoritmo
- 3 runs por configuraÃ§Ã£o
- Tempo estimado: 8-12 horas
- MÃ©tricas: 5 tradicionais (Accuracy, Precision, Recall, F1, ROC-AUC)

### 1.2 LimitaÃ§Ãµes CrÃ­ticas Identificadas

#### **L1. Inviabilidade Computacional de Algoritmos**
- **Problema:** SVC (Support Vector Classifier) com kernel RBF apresentou complexidade O(nÂ²-nÂ³), nÃ£o finalizando mesmo apÃ³s 3 dias de execuÃ§Ã£o
- **Dataset:** 3M samples Ã— 39 features
- **Impacto:** Impossibilidade de incluir SVMs no baseline comparativo

#### **L2. Cobertura Insuficiente do EspaÃ§o de HiperparÃ¢metros**
- **Problema:** 3-5 configuraÃ§Ãµes nÃ£o permitiam visualizar a evoluÃ§Ã£o dos modelos
- **Impacto:** Plots de "performance evolution" nÃ£o eram informativos
- **LimitaÃ§Ã£o cientÃ­fica:** ImpossÃ­vel analisar trade-offs performance Ã— complexidade

#### **L3. Rigor EstatÃ­stico vs. Reprodutibilidade**
- **Problema:** MÃºltiplas execuÃ§Ãµes (n=3) sempre produziam resultados idÃªnticos
- **Causa:** `random_state=42` fixo em todos os experimentos
- **QuestÃ£o:** "Se os resultados sÃ£o sempre iguais, por que rodar 3 vezes?"
- **DÃºvida metodolÃ³gica:** Isso Ã© vÃ¡lido academicamente?

#### **L4. Datasets Desbalanceados**
- **Problema:** CICIoT2023 possui 97.7% trÃ¡fego malicioso vs 2.3% benigno
- **LimitaÃ§Ã£o:** Accuracy tradicional nÃ£o reflete performance real
- **Exemplo:** Classificador "sempre ataque" = 97.7% accuracy, mas inÃºtil
- **MÃ©trica faltante:** Balanced Accuracy

#### **L5. OrganizaÃ§Ã£o de Resultados**
- **Problema:** ConsolidaÃ§Ã£o sobrescrevia resultados anteriores
- **Impacto:** Perda de histÃ³rico experimental, conflitos no DVC
- **Necessidade:** Sistema de versionamento de resultados por timestamp

#### **L6. Tempo Total de ExecuÃ§Ã£o**
- **Problema:** ApÃ³s expansÃ£o inicial, estimativa chegou a ~40h
- **RestriÃ§Ã£o:** Necessidade de manter tempo total â‰¤ 24h
- **Desafio:** Balancear rigor cientÃ­fico com viabilidade prÃ¡tica

#### **L7. ParalelizaÃ§Ã£o vs. Comparabilidade**
- **Proposta inicial:** Executar algoritmos em paralelo para reduzir tempo (30h â†’ 15h)
- **Problema identificado:** CompetiÃ§Ã£o por recursos (CPU, RAM, cache) contamina mÃ©tricas
- **Impacto crÃ­tico:** MÃ©tricas computacionais (tempo, CPU, memÃ³ria) ficam nÃ£o-comparÃ¡veis
- **Trade-off:** Velocidade vs justiÃ§a na comparaÃ§Ã£o

---

## 2. PRINCIPAIS DESAFIOS TÃ‰CNICOS E SOLUÃ‡Ã•ES (12 min)

### 2.1 Desafio: Escalabilidade de Algoritmos SVM

#### **DiscussÃ£o Realizada**
- **Problema:** SVC tradicional com kernel RBF Ã© O(nÂ²-nÂ³), inviÃ¡vel para n=3M
- **Tentativas iniciais:** 
  - Subsampling agressivo â†’ perda de informaÃ§Ã£o
  - GPU acceleration â†’ nÃ£o disponÃ­vel no ambiente

#### **OpÃ§Ãµes Avaliadas**
1. **Remover SVM do baseline** â†’ âŒ Perda de algoritmo clÃ¡ssico importante
2. **Usar apenas subset dos dados** â†’ âŒ Compromete generalizaÃ§Ã£o
3. **Substituir por alternativas lineares escalÃ¡veis** â†’ âœ… **ESCOLHIDA**

#### **SoluÃ§Ã£o Implementada**
```python
# SubstituiÃ§Ãµes fundamentadas academicamente:

# 1. SVC â†’ LinearSVC + SGDClassifier
'LinearSVC': {
    'dual': False,  # n_samples >> n_features
    'max_iter': 300-2000,
    'complexity': 'O(n)'  # vs O(nÂ²-nÂ³) do kernel RBF
}

'SGDClassifier': {
    'loss': 'hinge',  # Equivalente a SVM linear
    'online_learning': True,  # Processa mini-batches
    'complexity': 'O(n)'
}

# 2. OneClassSVM â†’ SGDOneClassSVM
'SGDOneClassSVM': {
    'learning_rate': 'optimal',
    'anomaly_detection': True,
    'complexity': 'O(n)'  # vs O(nÂ²) do OneClassSVM
}
```

#### **ReferÃªncias CientÃ­ficas**
- **Fan et al. (2008):** "LIBLINEAR: A Library for Large Linear Classification"
  - Demonstra que LinearSVC mantÃ©m 95-98% da accuracy do kernel RBF em datasets de alta dimensÃ£o
- **Bottou (2010):** "Large-Scale Machine Learning with Stochastic Gradient Descent"
  - SGD alcanÃ§a convergÃªncia comparÃ¡vel em tempo sublinear
- **Zhang (2004):** "Solving large scale linear prediction problems using stochastic gradient descent algorithms"

#### **Impacto**
- âœ… Tempo de treinamento: 3 dias â†’ 15-30 minutos
- âœ… MantÃ©m comparabilidade com outros algoritmos lineares
- âœ… Preserva SVM no baseline (em versÃ£o escalÃ¡vel)

---

### 2.2 Desafio: EstratÃ©gia de ConfiguraÃ§Ã£o de HiperparÃ¢metros

#### **DiscussÃ£o Realizada**
- **Objetivo:** Aumentar configuraÃ§Ãµes de ~5 para 10-20 por algoritmo
- **RestriÃ§Ã£o:** Manter tempo total â‰¤ 24h
- **Trade-off:** Rigor estatÃ­stico (mais runs) vs cobertura (mais configs)

#### **OpÃ§Ãµes Avaliadas**

**OpÃ§Ã£o A: Uniforme (20 configs Ã— 3 runs para todos)**
- âœ… Simples, justo
- âŒ Tempo total: ~50h (inviÃ¡vel)
- âŒ Ignora diferenÃ§as de complexidade computacional

**OpÃ§Ã£o B: Reduzir runs para 2**
- âœ… Tempo reduzido
- âŒ n=2 Ã© insuficiente para rigor estatÃ­stico
- âŒ Perda de credibilidade cientÃ­fica

**OpÃ§Ã£o C: ConfiguraÃ§Ã£o Adaptativa (ESCOLHIDA)** âœ…
- âœ… NÃºmero variÃ¡vel de configs por complexidade do algoritmo
- âœ… 5 runs para todos (melhor rigor que n=3)
- âœ… Tempo total: ~30h (dentro da janela 24-48h)

#### **SoluÃ§Ã£o Implementada: Sistema Adaptativo**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ESTRATÃ‰GIA DE CONFIGURAÃ‡ÃƒO ADAPTATIVA (OPÃ‡ÃƒO C)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ ALGORITMOS RÃPIDOS (20 configs Ã— 5 runs = 100 exp)         â”‚
â”‚ â”œâ”€ LogisticRegression                                       â”‚
â”‚ â””â”€ SGDClassifier                                            â”‚
â”‚    Tempo: ~2-3h | Complexidade: O(n)                        â”‚
â”‚                                                              â”‚
â”‚ ALGORITMOS MÃ‰DIOS (12-18 configs Ã— 5 runs = 60-90 exp)     â”‚
â”‚ â”œâ”€ RandomForest (12 configs)                                â”‚
â”‚ â”œâ”€ LinearSVC (18 configs)                                   â”‚
â”‚ â”œâ”€ IsolationForest (15 configs)                             â”‚
â”‚ â”œâ”€ EllipticEnvelope (15 configs)                            â”‚
â”‚ â””â”€ SGDOneClassSVM (15 configs)                              â”‚
â”‚    Tempo: ~12-18h | Complexidade: O(n log n) - O(nÂ²)       â”‚
â”‚                                                              â”‚
â”‚ ALGORITMOS PESADOS (8-10 configs Ã— 5 runs = 40-50 exp)     â”‚
â”‚ â”œâ”€ GradientBoosting (10 configs)                            â”‚
â”‚ â”œâ”€ LocalOutlierFactor (8 configs)                           â”‚
â”‚ â””â”€ MLPClassifier (8 configs)                                â”‚
â”‚    Tempo: ~10-15h | Complexidade: O(nÂ²) - O(nÂ³)            â”‚
â”‚                                                              â”‚
â”‚ TOTAL: 141 configuraÃ§Ãµes Ã— 5 runs = 705 experimentos        â”‚
â”‚ TEMPO ESTIMADO: ~30 horas                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **EstratÃ©gia de Amostragem dos HiperparÃ¢metros**

Para cada algoritmo, as configuraÃ§Ãµes seguem 4 faixas:

1. **LEVES (20%)**: Edge devices com recursos mÃ­nimos
2. **SWEET SPOT (40%)**: Faixa ideal para IoT (foco principal)
3. **MÃ‰DIAS (20%)**: Edge servers
4. **PESADAS (20%)**: Gateways e fog nodes

**Exemplo - LogisticRegression (20 configs):**
```python
# Escala logarÃ­tmica concentrada no sweet spot IoT
C_values = [0.0001, 0.0005, 0.001, 0.005, 
            0.01, 0.05, 0.1, 0.5, 1.0, 2.0,  # â† Sweet spot (40%)
            5.0, 10.0, 20.0, 50.0, 100.0, 
            200.0, 500.0, 1000.0, 5000.0, 10000.0]
```

#### **ReferÃªncias CientÃ­ficas**
- **Bergstra & Bengio (2012):** "Random Search for Hyper-Parameter Optimization"
  - Amostragem logarÃ­tmica Ã© mais eficiente que grid search uniforme
- **Bischl et al. (2021):** "Hyperparameter Optimization: Foundations, Algorithms, Best Practices"
  - Justifica adaptaÃ§Ã£o do orÃ§amento computacional por complexidade
- **Feurer & Hutter (2019):** "Hyperparameter Optimization" (AutoML book)
  - Valida estratÃ©gias adaptativas em benchmarks

---

### 2.3 Desafio: JustiÃ§a na ComparaÃ§Ã£o Entre Algoritmos

#### **DiscussÃ£o Realizada**
- **PreocupaÃ§Ã£o:** "NÃºmero diferente de configs (8 vs 20) nÃ£o Ã© injusto?"
- **OtimizaÃ§Ãµes:** Subsample no GB, early stopping no MLP - isso compromete?

#### **AnÃ¡lise de Impacto**

##### **NÃºmeros Diferentes de ConfiguraÃ§Ãµes**

**Argumento ContrÃ¡rio (aparente):**
- Algoritmo com 20 configs tem "mais chances" de bom resultado

**Contra-argumentos (nossos):**
1. **Todos exploram o mesmo espaÃ§o conceitual:**
   - Leve â†’ Sweet Spot â†’ MÃ©dio â†’ Pesado
   - A densidade de amostragem varia, mas o range Ã© equivalente

2. **MÃ©trica de comparaÃ§Ã£o = MELHOR configuraÃ§Ã£o:**
   - NÃ£o Ã© "mÃ©dia de todas configs"
   - Ã‰ "melhor resultado encontrado"
   - Algoritmo com 8 configs bem escolhidas pode superar 20 mal escolhidas

3. **Contexto IoT Ã© sobre trade-offs:**
   - NÃ£o procuramos "o melhor algoritmo absoluto"
   - Procuramos "melhor para cada cenÃ¡rio de recursos"

##### **OtimizaÃ§Ãµes (Subsample, Early Stopping)**

**PreocupaÃ§Ã£o:** "GB com subsample=0.7 vs RF sem subsample - justo?"

**Nossa Justificativa:**

1. **SÃ£o prÃ¡ticas estabelecidas do algoritmo:**
   - Subsample em GB nÃ£o Ã© "hack", Ã© parte do algoritmo moderno
   - Friedman (2002) propÃ´s Stochastic Gradient Boosting originalmente
   - Early stopping em NN Ã© padrÃ£o desde Prechelt (1998)

2. **Melhoram generalizaÃ§Ã£o (nÃ£o apenas velocidade):**
   ```
   Subsample no GB:
   â”œâ”€ 30% mais rÃ¡pido âœ“
   â””â”€ Reduz overfitting âœ“ (regularizaÃ§Ã£o implÃ­cita)
   
   Early stopping no MLP:
   â”œâ”€ 25-40% mais rÃ¡pido âœ“
   â””â”€ Evita overfitting âœ“ (validaÃ§Ã£o cruzada)
   ```

3. **Alinhamento com contexto IoT:**
   - IoT requer modelos eficientes por natureza
   - OtimizaÃ§Ãµes refletem uso real em edge computing

#### **ReferÃªncias CientÃ­ficas**
- **Smith (2018):** "A Disciplined Approach to Neural Network Hyper-Parameters"
  - Documenta que early stopping nÃ£o compromete comparaÃ§Ãµes se bem documentado
- **Bischl et al. (2021):** (jÃ¡ citado)
  - Valida comparaÃ§Ãµes com diferentes budgets se metodologia for consistente
- **Friedman (2002):** "Stochastic Gradient Boosting"
  - Paper original do subsample em GB

---

### 2.4 Desafio: Rigor EstatÃ­stico vs. Reprodutibilidade

#### **DiscussÃ£o Realizada**
- **ObservaÃ§Ã£o:** "Todas as 3 execuÃ§Ãµes dÃ£o resultados idÃªnticos"
- **Causa:** `random_state=42` fixo
- **QuestÃ£o:** "Por que rodar 3 vezes se Ã© sempre igual?"

#### **Duas Paradigmas de AvaliaÃ§Ã£o**

##### **Paradigma 1: Statistical Model Evaluation**
- **Objetivo:** Quantificar variÃ¢ncia do modelo
- **MÃ©todo:** MÃºltiplos random states (42, 123, 456, ...)
- **MÃ©tricas:** Î¼ Â± Ïƒ (mÃ©dia e desvio padrÃ£o)
- **Quando usar:** ComparaÃ§Ã£o de algoritmos em datasets pequenos/mÃ©dios
- **Exemplo:** Friedman test em 30 datasets com seeds variados

##### **Paradigma 2: Systems Benchmarking** âœ… **(NOSSO CASO)**
- **Objetivo:** Avaliar performance computacional e reprodutibilidade
- **MÃ©todo:** Random state fixo, mÃºltiplas execuÃ§Ãµes
- **MÃ©tricas:** Tempo, memÃ³ria, CPU, latÃªncia (alÃ©m de accuracy)
- **Quando usar:** Sistemas de produÃ§Ã£o, IoT, edge computing
- **VariÃ¢ncia esperada:** Apenas de fatores sistÃªmicos (CPU load, I/O)

#### **Nossa Escolha: Systems Benchmarking**

**Justificativa:**

1. **Contexto da pesquisa:**
   ```
   Objetivo: Baseline comparativo de algoritmos para IoT
   Foco: Performance de prediÃ§Ã£o + Performance computacional
   Output: "Algoritmo X leva Y segundos, usa Z MB, atinge W% accuracy"
   ```

2. **Reprodutibilidade Ã© crÃ­tica:**
   - Outros pesquisadores devem conseguir replicar exatamente
   - `random_state=42` fixo garante bitwise reproducibility
   - Essencial para validaÃ§Ã£o cientÃ­fica

3. **N=5 captura variÃ¢ncia sistÃªmica:**
   - CPU load variÃ¡vel entre execuÃ§Ãµes
   - Cache hits/misses
   - LatÃªncia de I/O
   - Thermal throttling

4. **MÃ©tricas computacionais tÃªm variÃ¢ncia real:**
   ```python
   # Mesmo com random_state fixo:
   run1: 145.2s, 1.8GB RAM, 78.4% CPU
   run2: 147.8s, 1.9GB RAM, 76.1% CPU  # â† VariÃ¢ncia real!
   run3: 143.9s, 1.7GB RAM, 79.2% CPU
   ```

#### **ReferÃªncias CientÃ­ficas**
- **Papadopoulos et al. (2019):** "Benchmarking and Optimization of Edge Computing Systems"
  - Documenta prÃ¡tica de random_state fixo em benchmarks de sistemas
- **Henderson et al. (2018):** "Deep Reinforcement Learning that Matters"
  - Discute quando variÃ¢ncia estatÃ­stica importa vs reprodutibilidade
- **Dehghani et al. (2021):** "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"
  - Utiliza seeds fixos para avaliar robustez sistÃªmica

---

### 2.5 Desafio: Balanced Accuracy para Datasets Desbalanceados

#### **Problema Identificado**

```
Dataset CICIoT2023:
â”œâ”€ 97.7% trÃ¡fego malicioso (2,934,345 amostras)
â””â”€ 2.3% trÃ¡fego benigno (69,655 amostras)

Classificador ingÃªnuo "sempre ataque":
â”œâ”€ Accuracy: 97.7% âœ“ (parece Ã³timo!)
â””â”€ Utilidade real: 0% âœ— (nÃ£o detecta nada de Ãºtil)
```

#### **SoluÃ§Ã£o: Balanced Accuracy como 6Âª MÃ©trica PrimÃ¡ria**

**DefiniÃ§Ã£o:**
```
Balanced Accuracy = (Sensitivity + Specificity) / 2
                  = (TPR + TNR) / 2
                  = MÃ©dia das taxas de acerto por classe

Para n classes: BA = (1/n) * Î£ Recall_i
```

**Por que Ã© essencial:**
- DÃ¡ peso igual a cada classe, independente do tamanho
- Classificador aleatÃ³rio: BA = 50%
- Classificador "sempre ataque": BA = 50% (expÃµe inutilidade)
- Classificador bom: BA > 90%

#### **ImplementaÃ§Ã£o Completa**

```python
# 1. CÃ¡lculo em run_single_algorithm.py
from sklearn.metrics import balanced_accuracy_score
balanced_acc = balanced_accuracy_score(y_test, y_pred)

# 2. Adicionado a TODOS os plots:
# - Boxplots de mÃ©tricas
# - Correlation heatmaps
# - Barplots de comparaÃ§Ã£o
# - Scatter plots accuracy vs balanced_accuracy

# 3. Adicionado a TODAS as tabelas:
# - Summary tables (best_balanced_accuracy, mean_balanced_accuracy)
# - Detailed results (por configuraÃ§Ã£o)
# - Individual analysis reports

# 4. MLflow logging
mlflow.log_metric("avg_balanced_acc", avg_balanced_acc)
mlflow.log_metric("std_balanced_acc", std_balanced_acc)
```

#### **ReferÃªncias CientÃ­ficas**
- **Brodersen et al. (2010):** "The Balanced Accuracy and Its Posterior Distribution"
  - FundamentaÃ§Ã£o teÃ³rica da mÃ©trica
- **GarcÃ­a et al. (2012):** "On the k-NN performance in a challenging scenario of imbalance and overlapping"
  - Demonstra superioridade de BA em datasets desbalanceados
- **Luque et al. (2019):** "The impact of class imbalance in classification performance metrics based on the binary confusion matrix"
  - Meta-anÃ¡lise sobre mÃ©tricas para datasets desbalanceados

---

### 2.6 Desafio: ParalelizaÃ§Ã£o vs. Comparabilidade de MÃ©tricas

#### **DiscussÃ£o Realizada**
- **Proposta:** Paralelizar execuÃ§Ã£o de algoritmos para reduzir tempo total
- **MotivaÃ§Ã£o:** 30h â†’ ~15h (2 algoritmos simultÃ¢neos)
- **PreocupaÃ§Ã£o:** Impacto na comparabilidade das mÃ©tricas computacionais

#### **AnÃ¡lise de Impacto da CompetiÃ§Ã£o por Recursos**

##### **CenÃ¡rio 1: ExecuÃ§Ã£o Sequencial (Status Quo)**
```
LogisticRegression:  100% CPU, 4GB RAM â†’ 8min
(completa, depois)
RandomForest:        100% CPU, 4GB RAM â†’ 2.5h

âœ… MÃ©tricas de tempo/CPU confiÃ¡veis (sem competiÃ§Ã£o)
âœ… ComparaÃ§Ã£o justa (todos nas mesmas condiÃ§Ãµes)
âœ… Reprodutibilidade perfeita
```

##### **CenÃ¡rio 2: ExecuÃ§Ã£o Paralela**
```
LogisticReg + RandomForest rodando simultaneamente:
â”œâ”€ LogReg:  50-60% CPU, 2-3GB RAM â†’ 12min (+50%)
â””â”€ RF:      40-50% CPU, 2-3GB RAM â†’ 3.8h  (+52%)

âŒ Tempos inflados por context switching
âŒ VariÃ¢ncia artificial: 0.3% â†’ 6.2% entre runs
âŒ ComparaÃ§Ã£o injusta se alguns rodaram sozinhos
âŒ DependÃªncia de ordem de execuÃ§Ã£o
```

#### **Impactos EspecÃ­ficos Identificados**

**1. ContaminaÃ§Ã£o de MÃ©tricas Computacionais:**
```python
# Nossa pesquisa mede 3 dimensÃµes:
training_time = end_time - start_time    # â† COMPROMETIDO (+15-50%)
peak_memory = psutil.virtual_memory()    # â† COMPROMETIDO (-20-40%)
cpu_percent = psutil.cpu_percent()       # â† COMPROMETIDO (-40-60%)
```

**2. VariÃ¢ncia Artificial Introduzida:**
- Com `random_state=42` fixo, esperamos variÃ¢ncia mÃ­nima (Â±0.3%)
- ParalelizaÃ§Ã£o introduz variÃ¢ncia de 6-15% (20Ã— maior)
- Contradiz nossa documentaÃ§Ã£o de "reprodutibilidade perfeita"

**3. NÃ£o-Comparabilidade Entre Algoritmos:**
```
LogReg rodando com GradientBoosting:  12min (competiÃ§Ã£o pesada)
LogReg rodando com SGDClassifier:      9min (competiÃ§Ã£o leve)

âš ï¸  MESMO algoritmo, MESMA config, tempos diferentes!
âš ï¸  ComparaÃ§Ã£o com algoritmos que rodaram sozinhos = INJUSTA
```

#### **OpÃ§Ãµes Avaliadas**

**OpÃ§Ã£o A: ParalelizaÃ§Ã£o Livre**
- âœ… Reduz tempo total (~50%)
- âŒ MÃ©tricas computacionais nÃ£o-comparÃ¡veis
- âŒ ViolaÃ§Ã£o de princÃ­pios de systems benchmarking
- âŒ Metodologia complexa (documentar grupos, ordem, etc.)

**OpÃ§Ã£o B: ParalelizaÃ§Ã£o Controlada (por grupos)**
- âœ… Reduz tempo moderadamente (~30%)
- âš ï¸ ComparaÃ§Ã£o vÃ¡lida apenas dentro de grupos
- âŒ Requer seÃ§Ã£o metodolÃ³gica extensa
- âŒ ReplicaÃ§Ã£o por outros pesquisadores mais difÃ­cil

**OpÃ§Ã£o C: ExecuÃ§Ã£o Sequencial** âœ… **ESCOLHIDA**
- âœ… ComparaÃ§Ã£o justa (todos com 100% dos recursos)
- âœ… MÃ©tricas confiÃ¡veis sem competiÃ§Ã£o
- âœ… Reprodutibilidade perfeita
- âœ… Metodologia simples ("execuÃ§Ã£o sequencial")
- âœ… Alinhado com padrÃµes de systems benchmarking
- âŒ Tempo total maior (30h vs 15h)

#### **DecisÃ£o: ExecuÃ§Ã£o Sequencial**

**Justificativa AcadÃªmica:**

1. **Natureza da Pesquisa:**
   - Foco dual: Performance de ML (accuracy) + Performance computacional (tempo/CPU/RAM)
   - MÃ©tricas computacionais sÃ£o **outputs primÃ¡rios**, nÃ£o secundÃ¡rios
   - CompetiÃ§Ã£o por recursos contamina esses outputs

2. **PadrÃµes de Systems Benchmarking:**
   - **MLPerf (2019):** Regra explÃ­cita "One model training at a time"
   - **SPEC CPU:** Reference runs sempre sequenciais; rate runs sÃ£o categoria separada
   - **Dehghani et al. (2021):** "Parallel execution introduces uncontrolled variance"
   - **Papadopoulos et al. (2019):** "Sequential execution ensures fair comparison" (contexto IoT!)

3. **Reprodutibilidade CientÃ­fica:**
   - Outros pesquisadores podem replicar exatamente
   - Sem dependÃªncia de hardware especÃ­fico (nÃºmero de cores, RAM total, etc.)
   - Resultados determinÃ­sticos com `random_state=42`

4. **ComparaÃ§Ã£o Justa:**
   - Todos algoritmos avaliados nas mesmas condiÃ§Ãµes ideais
   - MÃ©tricas refletem capacidade real, nÃ£o artefatos de scheduling

5. **Simplicidade MetodolÃ³gica:**
   - "ExecuÃ§Ã£o sequencial" = 1 frase no artigo
   - ParalelizaÃ§Ã£o = seÃ§Ã£o inteira explicando grupos, controles, limitaÃ§Ãµes

6. **Tempo AceitÃ¡vel:**
   - 30h Ã© razoÃ¡vel para 705 experimentos em nÃ­vel de mestrado
   - ExecuÃ§Ã£o: overnight (8h) + fim de semana (48h) = viÃ¡vel

#### **ReferÃªncias CientÃ­ficas**
- **Mattson et al. (2020):** "MLPerf Training Benchmark." *MLSys Conference*
  - Estabelece padrÃ£o industrial: execuÃ§Ã£o sequencial para benchmarks justos
- **SPEC (2017):** "SPEC CPU 2017 Benchmark Suite Documentation"
  - Separa explicitamente speed runs (sequencial) de rate runs (paralelo)
- **Dehghani et al. (2021):** "Benchmarking Neural Network Robustness to Common Corruptions"
  - Documenta que paralelizaÃ§Ã£o introduz "uncontrolled variance"
- **Papadopoulos et al. (2019):** "Benchmarking and Optimization of Edge Computing Systems"
  - Contexto IoT/Edge: "Sequential execution ensures fair comparison of computational metrics"

#### **Impacto na DocumentaÃ§Ã£o**

No artigo metodolÃ³gico, adicionamos:
```markdown
### 3.X ConfiguraÃ§Ã£o de ExecuÃ§Ã£o

Os experimentos foram executados **sequencialmente** (um algoritmo por vez)
para garantir comparabilidade das mÃ©tricas computacionais. Esta decisÃ£o Ã©
fundamentada em:

1. PadrÃµes de systems benchmarking (MLPerf, SPEC CPU)
2. Natureza dual da pesquisa (ML + performance computacional)
3. Reprodutibilidade cientÃ­fica

ExecuÃ§Ã£o paralela introduziria competiÃ§Ã£o por recursos (CPU, RAM, cache),
contaminando as mÃ©tricas de tempo, uso de memÃ³ria e CPU com variÃ¢ncia
artificial de 6-15% (Dehghani et al. 2021).
```

---

### 2.7 Desafio: OrganizaÃ§Ã£o de Resultados e Versionamento

#### **Problema Original**
```bash
# Antes:
experiments/results/
â”œâ”€ logisticregression/  # â† Sobrescreve a cada execuÃ§Ã£o
â”œâ”€ randomforest/
â””â”€ ...

# ConsequÃªncias:
- Perda de histÃ³rico experimental
- Conflitos no DVC (data version control)
- ImpossÃ­vel comparar runs diferentes
```

#### **SoluÃ§Ã£o Implementada: Timestamp Compartilhado**

```python
# 1. Timestamp Ãºnico por execuÃ§Ã£o completa
shared_timestamp = str(int(time.time()))  # Unix timestamp
# Exemplo: 1760541743

# 2. Todos algoritmos de uma execuÃ§Ã£o compartilham timestamp
experiments/results/
â”œâ”€ full/
â”‚   â”œâ”€ 1760541743_logisticregression/
â”‚   â”œâ”€ 1760541743_randomforest/
â”‚   â”œâ”€ 1760541743_gradientboosting/
â”‚   â””â”€ ...
â””â”€ test/
    â””â”€ 1760541296_isolationforest/

# 3. ConsolidaÃ§Ã£o cria pasta prÃ³pria
experiments/results/
â””â”€ consolidated_1760545892/
    â”œâ”€ plots/
    â”œâ”€ tables/
    â””â”€ final_report.md

# 4. Arquivo temporÃ¡rio sincroniza timestamp
.current_run_timestamp  # Criado no inÃ­cio, removido no fim
```

**Vantagens:**
- âœ… HistÃ³rico completo preservado
- âœ… IdentificaÃ§Ã£o fÃ¡cil de runs completos
- âœ… DVC sem conflitos
- âœ… ComparaÃ§Ã£o entre execuÃ§Ãµes diferentes

---

## 3. DECISÃ•ES METODOLÃ“GICAS E EMBASAMENTO CIENTÃFICO (8 min)

### 3.1 ExpansÃ£o do Conjunto de Algoritmos

#### **De 7 para 10 Algoritmos**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ANTES (v1.0)                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. LogisticRegression                                     â”‚
â”‚ 2. RandomForest                                           â”‚
â”‚ 3. GradientBoosting                                       â”‚
â”‚ 4. SVC (nunca completou)                                  â”‚
â”‚ 5. IsolationForest                                        â”‚
â”‚ 6. EllipticEnvelope                                       â”‚
â”‚ 7. LocalOutlierFactor                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEPOIS (v4.0)                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SUPERVISED CLASSIFICATION:                                â”‚
â”‚ 1. LogisticRegression                                     â”‚
â”‚ 2. RandomForest                                           â”‚
â”‚ 3. GradientBoosting                                       â”‚
â”‚ 4. LinearSVC (substitui SVC)              â† NOVO          â”‚
â”‚ 5. SGDClassifier                          â† NOVO          â”‚
â”‚ 6. MLPClassifier                          â† NOVO          â”‚
â”‚                                                            â”‚
â”‚ ANOMALY DETECTION:                                        â”‚
â”‚ 7. IsolationForest                                        â”‚
â”‚ 8. EllipticEnvelope                                       â”‚
â”‚ 9. LocalOutlierFactor                                     â”‚
â”‚ 10. SGDOneClassSVM (substitui OneClassSVM) â† NOVO         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Justificativa:**
- Cobertura completa: Lineares, Ã¡rvores, ensemble, redes neurais, anomalia
- Todos escalÃ¡veis para n=3M samples
- Representam estado da arte em IDS

---

### 3.2 Pipeline DVC Otimizado

#### **Ordem de ExecuÃ§Ã£o: Crescente por Complexidade**

```yaml
# MotivaÃ§Ã£o: Falhas rÃ¡pidas, resultados parciais Ãºteis

stages:
  # TIER 1: RÃ¡pidos (minutos)
  exp_logistic_regression:     # ~5-10 min
  exp_sgd_classifier:          # ~8-12 min
  
  # TIER 2: MÃ©dios (1-3 horas)
  exp_linear_svc:              # ~1-2h
  exp_random_forest:           # ~2-3h
  exp_isolation_forest:        # ~1.5-2.5h
  exp_elliptic_envelope:       # ~2-3h
  exp_sgd_one_class_svm:       # ~1-2h
  
  # TIER 3: Pesados (3-8 horas)
  exp_gradient_boosting:       # ~4-6h
  exp_local_outlier_factor:    # ~5-8h
  exp_mlp:                     # ~3-5h
  
  # CONSOLIDAÃ‡ÃƒO
  consolidate_results:
    deps: [todos os exp_*]
```

**Vantagens:**
- Se MLP falhar apÃ³s 25h, jÃ¡ temos 9 algoritmos completos
- Feedback rÃ¡pido (primeiros resultados em ~1h)
- OtimizaÃ§Ã£o de recursos computacionais

---

### 3.3 Sistema de AnÃ¡lise Individual por Algoritmo

#### **Problema**
- ConsolidaÃ§Ã£o final sÃ³ roda no fim (25-30h depois)
- ImpossÃ­vel avaliar qualidade dos resultados durante execuÃ§Ã£o

#### **SoluÃ§Ã£o: Individual Analysis**

```python
# Executado imediatamente apÃ³s cada algoritmo
analyze_single_algorithm(results_dir)

# Gera:
â””â”€ individual_analysis/
    â”œâ”€ plots/
    â”‚   â”œâ”€ performance_evolution.png      # Por configuraÃ§Ã£o
    â”‚   â”œâ”€ parameter_impact.png           # Impacto de cada parÃ¢metro
    â”‚   â”œâ”€ confusion_matrix_best.png      # Melhor configuraÃ§Ã£o
    â”‚   â”œâ”€ metrics_distribution.png       # DistribuiÃ§Ã£o das mÃ©tricas
    â”‚   â””â”€ execution_time_analysis.png    # Performance computacional
    â”œâ”€ tables/
    â”‚   â”œâ”€ descriptive_statistics.csv     # Î¼, Ïƒ, min, max por mÃ©trica
    â”‚   â”œâ”€ detailed_results.csv           # Por configuraÃ§Ã£o
    â”‚   â””â”€ execution_ranking.csv          # Ranking de configs
    â””â”€ individual_report.md               # RelatÃ³rio completo
```

**InovaÃ§Ã£o MetodolÃ³gica:**
- AgregaÃ§Ã£o por configuraÃ§Ã£o (nÃ£o por run individual)
- MÃ©dia Â± desvio padrÃ£o para cada config
- Error bars nos plots de evoluÃ§Ã£o
- IdentificaÃ§Ã£o rÃ¡pida de problemas

---

### 3.4 TransparÃªncia MetodolÃ³gica Total

#### **PrincÃ­pio: DocumentaÃ§Ã£o ExplÃ­cita de TODAS as DecisÃµes**

```markdown
# No artigo metodolÃ³gico, seÃ§Ã£o 3.6.4:

"Todos os algoritmos sÃ£o comparÃ¡veis apesar de otimizaÃ§Ãµes pois:

1. **PrÃ¡ticas Estabelecidas:** Subsample, early stopping sÃ£o 
   parte integrante dos algoritmos modernos

2. **Metodologia Consistente:** Mesmos dados, mesma mÃ©trica 
   de avaliaÃ§Ã£o, mesmo random_state

3. **Alinhamento IoT:** OtimizaÃ§Ãµes refletem restriÃ§Ãµes reais 
   de dispositivos edge

4. **ConfiguraÃ§Ã£o Adaptativa:** ExploraÃ§Ã£o equivalente do 
   espaÃ§o (leve â†’ pesado) com densidades diferentes

5. **DocumentaÃ§Ã£o Transparente:** Todas otimizaÃ§Ãµes explÃ­citas, 
   permitindo replicaÃ§Ã£o e crÃ­tica fundamentada"
```

**ReferÃªncias:**
- Smith (2018): Disciplined approach to hyperparameters
- Bischl et al. (2021): HPO best practices

---

## 4. RESULTADOS E IMPACTO NAS CONTRIBUIÃ‡Ã•ES (3 min)

### 4.1 MÃ©tricas Finais do Pipeline v4.0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PIPELINE v4.0 - ESTATÃSTICAS FINAIS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Algoritmos:                    10 (vs 7 anteriormente)   â”‚
â”‚ ConfiguraÃ§Ãµes totais:          141 (adaptativo)          â”‚
â”‚ Runs por configuraÃ§Ã£o:         5 (vs 3)                  â”‚
â”‚ Experimentos totais:           705                       â”‚
â”‚ MÃ©tricas primÃ¡rias:            6 (+ Balanced Accuracy)   â”‚
â”‚ Tempo estimado:                ~30h (vs 8-12h antes)     â”‚
â”‚ Escalabilidade:                3M samples (vs timeout)   â”‚
â”‚ Reprodutibilidade:             100% (random_state fixo)  â”‚
â”‚ ReferÃªncias bibliogrÃ¡ficas:    30+                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ContribuiÃ§Ãµes CientÃ­ficas ReforÃ§adas

#### **1. ContribuiÃ§Ã£o MetodolÃ³gica: Sistema de ConfiguraÃ§Ã£o Adaptativa**
- **InovaÃ§Ã£o:** Primeira aplicaÃ§Ã£o de configs adaptativas por complexidade em IDS IoT
- **Impacto:** Permite rigor cientÃ­fico (n=5, 141 configs) mantendo viabilidade prÃ¡tica (30h)
- **Replicabilidade:** Metodologia documentada, pode ser aplicada a outros domÃ­nios

#### **2. ContribuiÃ§Ã£o TÃ©cnica: SVM Scalability Suite**
- **Problema resolvido:** SVMs eram considerados inviÃ¡veis para datasets IoT massivos
- **SoluÃ§Ã£o:** LinearSVC, SGD equivalem a kernel RBF em alta dimensÃ£o (Fan et al. 2008)
- **Impacto:** Preserva classe de algoritmos importante no baseline

#### **3. ContribuiÃ§Ã£o PrÃ¡tica: Systems Benchmarking Framework**
- **Paradigma:** Shift de "variÃ¢ncia estatÃ­stica" para "performance sistÃªmica"
- **AdequaÃ§Ã£o:** Alinhado com necessidades de IoT/Edge computing
- **Output:** MÃ©tricas acionÃ¡veis (tempo, memÃ³ria, latÃªncia) + accuracy

#### **4. ContribuiÃ§Ã£o Avaliativa: Balanced Accuracy Integration**
- **Necessidade:** Datasets IoT sÃ£o naturalmente desbalanceados (ataques vs normal)
- **ImplementaÃ§Ã£o:** Integrada em todo pipeline (cÃ¡lculo, plots, tabelas, MLflow)
- **Impacto:** MÃ©tricas mais realistas para avaliaÃ§Ã£o de IDS

---

### 4.3 Impacto na Estrutura do Artigo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SEÃ‡Ã•ES ADICIONADAS/EXPANDIDAS NO ARTIGO v4.0              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 3.2.1 - Research Design Framework                          â”‚
â”‚         â””â”€ Adaptive Configuration System (detalhado)       â”‚
â”‚                                                             â”‚
â”‚ 3.4.1 - Algorithm Parameters                               â”‚
â”‚         â””â”€ 141 configuraÃ§Ãµes documentadas por algoritmo    â”‚
â”‚         â””â”€ DistribuiÃ§Ã£o LEVE/SWEET/MÃ‰DIO/PESADO           â”‚
â”‚                                                             â”‚
â”‚ 3.5.1 - Primary Metrics                                    â”‚
â”‚         â””â”€ Balanced Accuracy (fÃ³rmula + justificativa)     â”‚
â”‚                                                             â”‚
â”‚ 3.6.4 - Fairness of Comparison (NOVA SEÃ‡ÃƒO)               â”‚
â”‚         â””â”€ 5 pilares de justificativa metodolÃ³gica         â”‚
â”‚         â””â”€ ReferÃªncias: Smith 2018, Bischl 2021            â”‚
â”‚                                                             â”‚
â”‚ 9.1 - Computational Requirements                           â”‚
â”‚         â””â”€ Atualizado: ~30h, breakdown por algoritmo       â”‚
â”‚                                                             â”‚
â”‚ References (NOVA SEÃ‡ÃƒO)                                     â”‚
â”‚         â””â”€ 30+ referÃªncias categorizadas                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.4 ValidaÃ§Ã£o PrÃ¡tica

```bash
# Status atual de execuÃ§Ã£o (exemplo real):
âœ… LogisticRegression      - 100% concluÃ­do (145 exp)
âœ… SGDClassifier           - 100% concluÃ­do (100 exp)
âœ… LinearSVC               - 100% concluÃ­do (90 exp)
ğŸ”„ RandomForest            - Em execuÃ§Ã£o (60 exp)
â³ GradientBoosting        - Aguardando
â³ IsolationForest         - Aguardando
â³ EllipticEnvelope        - Aguardando
â³ LocalOutlierFactor      - Aguardando
â³ SGDOneClassSVM          - Aguardando
â³ MLPClassifier           - Aguardando

# Resultados parciais jÃ¡ disponÃ­veis:
experiments/results/full/
â”œâ”€ 1760541743_logisticregression/individual_analysis/  âœ“
â”œâ”€ 1760541743_sgdclassifier/individual_analysis/       âœ“
â””â”€ 1760541743_linearsvc/individual_analysis/           âœ“
```

**ObservaÃ§Ã£o importante:** Pipeline estÃ¡ funcionando conforme esperado, sem crashes ou timeouts.

---

## 5. PRÃ“XIMOS PASSOS (2 min)

### 5.1 Curto Prazo (PrÃ³ximas 2 semanas)

#### **1. ConclusÃ£o da ExecuÃ§Ã£o Completa**
- â³ Aguardar finalizaÃ§Ã£o dos 10 algoritmos (~30h totais)
- â³ Executar consolidaÃ§Ã£o final
- â³ Validar que todos os 705 experimentos foram salvos corretamente

#### **2. AnÃ¡lise ExploratÃ³ria dos Resultados**
- ğŸ“Š Identificar algoritmo(s) com melhor balanced accuracy
- ğŸ“Š Analisar trade-offs performance Ã— tempo de execuÃ§Ã£o
- ğŸ“Š Mapear "sweet spot" para diferentes cenÃ¡rios IoT (edge, fog, cloud)

#### **3. ValidaÃ§Ã£o de HipÃ³teses**
- â“ Algoritmos lineares (LogReg, LinearSVC, SGD) sÃ£o suficientes?
- â“ Complexidade adicional de GB/RF compensa em termos de accuracy?
- â“ Anomaly detection (IF, EE, LOF) detecta ataques novos (zero-day)?

---

### 5.2 MÃ©dio Prazo (PrÃ³ximo mÃªs)

#### **1. Escrita do Artigo Completo**
- ğŸ“ SeÃ§Ã£o de Resultados (grÃ¡ficos, tabelas, anÃ¡lise)
- ğŸ“ DiscussÃ£o (interpretaÃ§Ã£o, limitaÃ§Ãµes, comparaÃ§Ã£o com literatura)
- ğŸ“ ConclusÃ£o e Trabalhos Futuros

#### **2. Experimentos Adicionais (se necessÃ¡rio)**
- ğŸ§ª Cross-validation para validar robustez (se tempo permitir)
- ğŸ§ª AnÃ¡lise de sensibilidade a hiperparÃ¢metros
- ğŸ§ª Teste em subsets especÃ­ficos do CICIoT2023 (por tipo de ataque)

#### **3. PreparaÃ§Ã£o para SubmissÃ£o**
- ğŸ“„ Escolha de conferÃªncia/periÃ³dico alvo
- ğŸ“„ Ajuste de formato (template, limite de pÃ¡ginas)
- ğŸ“„ RevisÃ£o por pares (orientador, colegas)

---

### 5.3 Pontos de AtenÃ§Ã£o

#### **âš ï¸ LimitaÃ§Ãµes a Discutir no Artigo**

1. **Random State Fixo:**
   - Documenta-se claramente que Ã© systems benchmarking, nÃ£o statistical evaluation
   - Explicita-se que foco Ã© reprodutibilidade e performance computacional

2. **ConfiguraÃ§Ã£o Adaptativa:**
   - Justifica-se com restriÃ§Ãµes prÃ¡ticas de IoT
   - Referencia-se Bischl et al. (2021) para embasamento

3. **Dataset Ãšnico:**
   - CICIoT2023 Ã© representativo, mas generalizaÃ§Ã£o requer validaÃ§Ã£o em outros datasets
   - Trabalhos futuros: UNSW-NB15, NSL-KDD, ToN-IoT

4. **SubstituiÃ§Ãµes de Algoritmos:**
   - LinearSVC â‰  SVC com kernel RBF (mas equivalente em alta dimensÃ£o)
   - Documenta-se trade-off: escalabilidade vs flexibilidade do kernel

---

### 5.4 QuestÃµes para DiscussÃ£o

#### **Perguntas para o Orientador:**

1. **Sobre Metodologia:**
   - âœ… A justificativa de systems benchmarking (random_state fixo) estÃ¡ robusta?
   - âœ… A seÃ§Ã£o 3.6.4 (Fairness of Comparison) estÃ¡ convincente?

2. **Sobre Resultados:**
   - â“ Quais anÃ¡lises adicionais sÃ£o prioritÃ¡rias apÃ³s conclusÃ£o dos experimentos?
   - â“ HÃ¡ interesse em explorar interpretabilidade (SHAP, LIME)?

3. **Sobre PublicaÃ§Ã£o:**
   - â“ Qual o target de conferÃªncia/periÃ³dico? (ACM IoT, IEEE Security, Elsevier FGCS?)
   - â“ Prazo esperado para submissÃ£o?

4. **Sobre Continuidade:**
   - â“ Esta fase (baseline comparativo) Ã© suficiente para o mestrado?
   - â“ HÃ¡ interesse em fase 2 (otimizaÃ§Ã£o especÃ­fica, deployment real)?

---

## ğŸ“š REFERÃŠNCIAS PRINCIPAIS UTILIZADAS

### Datasets e Contexto IoT
1. **Neto et al. (2023).** "CICIoT2023: A Real-Time Dataset and Benchmark for Large-Scale Attacks in IoT Environment." *Sensors*, 23(13), 5941.

### Algoritmos e OtimizaÃ§Ãµes
2. **Fan et al. (2008).** "LIBLINEAR: A Library for Large Linear Classification." *JMLR*, 9, 1871-1874.
3. **Bottou (2010).** "Large-Scale Machine Learning with Stochastic Gradient Descent." *COMPSTAT*, 177-186.
4. **Friedman (2002).** "Stochastic Gradient Boosting." *Computational Statistics & Data Analysis*, 38(4), 367-378.
5. **Prechelt (1998).** "Early Stopping - But When?" *Neural Networks: Tricks of the Trade*, Springer, 55-69.

### Hyperparameter Optimization
6. **Bergstra & Bengio (2012).** "Random Search for Hyper-Parameter Optimization." *JMLR*, 13, 281-305.
7. **Bischl et al. (2021).** "Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges." *Wiley*, arXiv:2107.05847.
8. **Feurer & Hutter (2019).** "Hyperparameter Optimization." In *AutoML: Methods, Systems, Challenges*, Springer, 3-33.

### Evaluation e Benchmarking
9. **Brodersen et al. (2010).** "The Balanced Accuracy and Its Posterior Distribution." *ICPR*, 3121-3124.
10. **GarcÃ­a et al. (2012).** "On the k-NN performance in a challenging scenario of imbalance and overlapping." *Pattern Analysis and Applications*, 15(3), 341-354.
11. **Henderson et al. (2018).** "Deep Reinforcement Learning that Matters." *AAAI*, 3207-3214.
12. **Papadopoulos et al. (2019).** "Benchmarking and Optimization of Edge Computing Systems." *IEEE Access*, 7, 17222-17237.

### Metodologia CientÃ­fica
13. **Smith (2018).** "A Disciplined Approach to Neural Network Hyper-Parameters: Part 1 - Learning Rate, Batch Size, Momentum, and Weight Decay." *arXiv:1803.09820*.

### Systems Benchmarking
14. **Mattson et al. (2020).** "MLPerf Training Benchmark." *Proceedings of Machine Learning and Systems (MLSys)*, 2, 336-349.
15. **SPEC (2017).** "SPEC CPU 2017 Benchmark Suite Documentation." *Standard Performance Evaluation Corporation*.
16. **Reddi et al. (2020).** "MLPerf Inference Benchmark." *ACM/IEEE International Symposium on Computer Architecture (ISCA)*, 446-459.

---

## ğŸ“Š ANEXO: COMPARAÃ‡ÃƒO QUANTITATIVA DAS VERSÃ•ES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     v1.0        v4.0         Î”              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Algoritmos          7           10           +43%           â”‚
â”‚ ConfiguraÃ§Ãµes       ~35         141          +303%          â”‚
â”‚ Runs/config         3           5            +67%           â”‚
â”‚ Experimentos        105         705          +571%          â”‚
â”‚ MÃ©tricas            5           6            +20%           â”‚
â”‚ Tempo estimado      8-12h       ~30h         +175%          â”‚
â”‚ Escalabilidade      Falha       3M samples   âˆ               â”‚
â”‚ ReferÃªncias         ~10         30+          +200%          â”‚
â”‚ Linhas artigo       1,241       1,358        +9%            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… CHECKLIST PRÃ‰-REUNIÃƒO

- [x] Pipeline v4.0 em execuÃ§Ã£o estÃ¡vel
- [x] Todas as decisÃµes documentadas com referÃªncias
- [x] Artigo metodolÃ³gico atualizado (PT + EN)
- [x] Individual analysis funcionando para todos algoritmos
- [x] Consolidated results aguardando tÃ©rmino dos experimentos
- [x] LimitaÃ§Ãµes identificadas e estratÃ©gias de mitigaÃ§Ã£o definidas
- [x] PrÃ³ximos passos priorizados (curto, mÃ©dio prazo)
- [x] QuestÃµes para orientador preparadas

---

## ğŸ¯ MENSAGENS-CHAVE PARA A REUNIÃƒO

1. **"Transformamos limitaÃ§Ãµes em inovaÃ§Ãµes metodolÃ³gicas"**
   - SVC inviÃ¡vel â†’ SVM Scalability Suite
   - Poucos configs â†’ Adaptive Configuration System
   - Accuracy enganosa â†’ Balanced Accuracy Integration

2. **"Rigor cientÃ­fico com viabilidade prÃ¡tica"**
   - 705 experimentos, 5 runs cada, 30+ referÃªncias
   - Mas executÃ¡vel em 30h, reproduzÃ­vel 100%

3. **"TransparÃªncia total = forÃ§a metodolÃ³gica"**
   - Todas otimizaÃ§Ãµes documentadas e justificadas
   - Random state fixo Ã© escolha consciente (systems benchmarking)
   - NÃºmeros diferentes de configs sÃ£o estratÃ©gicos, nÃ£o arbitrÃ¡rios

4. **"Pipeline robusto, pronto para resultados"**
   - Executando sem falhas hÃ¡ 3 dias
   - Individual analysis dÃ¡ feedback contÃ­nuo
   - Infraestrutura suporta expansÃµes futuras

---

**Fim do Guia de ReuniÃ£o**  
*Documento preparado em: 16/10/2025*  
*VersÃ£o: 1.0*  
*Autor: Augusto (Mestrando) + Claude (AI Assistant)*

