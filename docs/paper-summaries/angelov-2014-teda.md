# Fichamento: Outside the Box - An Alternative Data Analytics Framework

**ReferÃªncia completa:** Angelov, P. (2014). "Outside the box: an alternative data analytics framework." *Journal of Automation, Mobile Robotics and Intelligent Systems*, 8(2), pp.29-35. DOI: 10.14313/JAMRIS_2-2014/16

**Data de leitura:** 2026-01-03
**Ãrea:** Machine Learning (Clustering Evolutivo)
**PDF:** https://www.jamris.org/index.php/JAMRIS/article/view/299/299

---

## 1. Objetivo do Artigo

Propor um framework alternativo para anÃ¡lise de dados chamado **TEDA (Typicality and Eccentricity Data Analytics)** que:
- NÃ£o depende das suposiÃ§Ãµes da teoria de probabilidade tradicional
- Ã‰ baseado em conceitos espaciais de **eccentricidade** e **tipicalidade**
- Pode ser usado para detecÃ§Ã£o de anomalias, clustering, classificaÃ§Ã£o, prediÃ§Ã£o e controle

---

## 2. MotivaÃ§Ã£o

### 2.1 CrÃ­tica Ã  Teoria de Probabilidade Tradicional (Frequentista)

**O que Ã© abordagem frequentista:**
A probabilidade Ã© definida como a **frequÃªncia relativa** de um evento quando o experimento Ã© repetido infinitas vezes.

```
P(evento) = lim(nâ†’âˆ) [nÃºmero de ocorrÃªncias / n tentativas]
```

**Exemplo:** P(cara) = 0.5 significa "se lanÃ§ar infinitas vezes, metade serÃ¡ cara".

**TrÃªs suposiÃ§Ãµes problemÃ¡ticas** (linhas 74-77):

| SuposiÃ§Ã£o | O que exige | Problema em dados reais |
|-----------|-------------|------------------------|
| IndependÃªncia | Amostras nÃ£o se influenciam | Temperatura hoje depende de ontem |
| n â†’ âˆ | Muitas observaÃ§Ãµes | Ã€s vezes temos apenas 10-50 dados |
| DistribuiÃ§Ã£o conhecida | Assumir Gaussiana, etc. | Dados reais raramente sÃ£o "puros" |

### 2.2 CrÃ­tica Ã s Abordagens Alternativas

#### Belief Functions (Teoria de Dempster-Shafer)
- Atribui **graus de crenÃ§a** a conjuntos de eventos
- Permite expressar incerteza parcial (ex: "60% de crenÃ§a que Ã© ataque OU falha")
- **Problema:** Requer especialista para definir os graus â€” Ã© subjetivo

#### Possibility Theory (Teoria da Possibilidade)
- Distingue entre **possibilidade** (Î ) e **necessidade** (N)
- Î (A) = "quÃ£o possÃ­vel Ã© A?" â€” pode somar mais que 1
- N(A) = "quÃ£o certo Ã© A?" â€” N(A) = 1 - Î (nÃ£o A)
- **Problema:** TambÃ©m requer definiÃ§Ã£o subjetiva por especialistas

| Abordagem | Pergunta | Soma |
|-----------|----------|------|
| Probabilidade | "Qual a chance?" | = 1 (obrigatÃ³rio) |
| Possibilidade | "Ã‰ compatÃ­vel?" | â‰¤ n (livre) |
| Necessidade | "Ã‰ certo?" | â‰¤ 1 |

#### First Principles Models (Modelos de Primeiros PrincÃ­pios)
- Derivados de **leis fundamentais** (fÃ­sica, quÃ­mica, etc.)
- Exemplo: y(t) = vâ‚€Â·sin(Î¸)Â·t - Â½Â·gÂ·tÂ² (movimento de projÃ©til)
- **Problema:** NÃ£o existe "Lei de Newton" para prever comportamento de rede

#### Expert-Based Models (Modelos Baseados em Especialistas)
- ConstruÃ­dos com **conhecimento humano** (regras, heurÃ­sticas)
- Exemplo: "SE pacotes > 10000/s ENTÃƒO alerta DDoS"
- **Problemas:** Subjetivo, trabalhoso, incompleto, nÃ£o se adapta a mudanÃ§as

### 2.3 O que TEDA PropÃµe

> "The proposed new framework TEDA is a systematic methodology which does not require prior assumptions" (linhas 44-45)

**"No prior assumptions or kernels"** significa:
- **Sem prior assumptions:** NÃ£o assume distribuiÃ§Ã£o (Gaussiana, etc.) antes de ver os dados
- **Sem kernels:** NÃ£o precisa escolher funÃ§Ã£o nÃºcleo nem bandwidth para estimar densidade

TEDA calcula tipicalidade **diretamente das distÃ¢ncias** entre os dados, sem escolhas arbitrÃ¡rias.

---

## 3. Conceitos Fundamentais

### 3.1 Accumulated Proximity (Ï€)

**DefiniÃ§Ã£o:** Soma das distÃ¢ncias de um ponto para todos os outros.

**FÃ³rmula:**
```
Ï€_j^k = Î£(i=1 to k) d_ij    onde d Ã© distÃ¢ncia (Euclidean, Mahalonobis, etc.)
```

### 3.2 Eccentricity (Î¾)

**DefiniÃ§Ã£o:** ProporÃ§Ã£o normalizada da proximidade acumulada â€” mede quÃ£o "excÃªntrico" (longe dos outros) um ponto Ã©.

**FÃ³rmula:**
```
Î¾_j^k = (2 Ã— Ï€_j^k) / Î£(i=1 to k) Ï€_i^k
```

**InterpretaÃ§Ã£o:**
- Î¾ alto â†’ ponto estÃ¡ **longe** dos outros â†’ **anÃ´malo**
- Î¾ baixo â†’ ponto estÃ¡ **perto** dos outros â†’ **tÃ­pico**
- Anomalia quando Î¾ > 1/k

### 3.3 Typicality (Ï„)

**DefiniÃ§Ã£o:** Complemento da eccentricidade â€” mede quÃ£o "tÃ­pico" um ponto Ã©.

**FÃ³rmula:**
```
Ï„_j^k = 1 - Î¾_j^k
```

**InterpretaÃ§Ã£o:**
- Ï„ alto â†’ ponto Ã© **tÃ­pico** (prÃ³ximo ao padrÃ£o)
- Ï„ baixo â†’ ponto Ã© **atÃ­pico** (candidato a anomalia)
- TÃ­pico quando Ï„ > 1/k

### 3.4 RelaÃ§Ã£o entre Eccentricity e Typicality

```
Ï„ = 1 - Î¾

Î£Î¾ = 2        (soma das eccentricidades)
Î£Ï„ = k - 2    (soma das tipicalidades)

0 < Î¾ < 1
0 < Ï„ < 1
```

### 3.5 "Builds Upon Mutual Dependence"

Diferente da probabilidade que **ignora** relaÃ§Ãµes entre amostras, TEDA **usa** essas relaÃ§Ãµes:

| Abordagem | Como trata as amostras |
|-----------|------------------------|
| Probabilidade | Cada amostra Ã© independente â€” conta apenas frequÃªncia |
| TEDA | Cada amostra influencia as outras â€” mede distÃ¢ncias entre elas |

**Exemplo:** Dados {10, 12, 11, 25}
- Probabilidade: P(cada) = 1/4 (todos iguais)
- TEDA: Ï„(10,12,11) alto (prÃ³ximos), Ï„(25) baixo (longe) â†’ 25 Ã© anÃ´malo

A "dependÃªncia mÃºtua" Ã© a **estrutura espacial** dos dados â€” quem estÃ¡ perto de quem.

### 3.6 MÃ©tricas de DistÃ¢ncia

O paper menciona (linhas 148-149): "This distance/proximity measure can be of any form, e.g. **Euclidean, Mahalonobis, cosine, Manhattan/city/L1**, etc."

#### 3.6.1 Euclidean Distance (DistÃ¢ncia Euclidiana)

**O que Ã©:** DistÃ¢ncia "em linha reta" entre dois pontos.

**FÃ³rmula:**
```
d(A, B) = âˆš[Î£áµ¢ (aáµ¢ - báµ¢)Â²]
```

**Exemplo:** A=(1,2), B=(4,6) â†’ d = âˆš[(3)Â² + (4)Â²] = 5

**Quando usar:**
- Features na mesma escala
- Dados contÃ­nuos
- DistÃ¢ncia padrÃ£o no TEDA

**LimitaÃ§Ã£o:** SensÃ­vel a escala; nÃ£o considera correlaÃ§Ã£o entre features.

#### 3.6.2 Manhattan Distance (L1 / City Block)

**O que Ã©:** Soma dos deslocamentos em cada eixo â€” como um tÃ¡xi em grid.

**FÃ³rmula:**
```
d(A, B) = Î£áµ¢ |aáµ¢ - báµ¢|
```

**Exemplo:** A=(1,2), B=(4,6) â†’ d = |3| + |4| = 7

**Quando usar:**
- Dados esparsos (muitos zeros)
- Robustez a outliers
- Features independentes

#### 3.6.3 Mahalanobis Distance

**O que Ã©:** DistÃ¢ncia que considera **correlaÃ§Ã£o** entre variÃ¡veis â€” mede desvios padrÃ£o do centro ajustando pela forma da distribuiÃ§Ã£o.

**FÃ³rmula:**
```
d(x, Î¼) = âˆš[(x - Î¼)áµ€ Â· Î£â»Â¹ Â· (x - Î¼)]
```
Onde Î£â»Â¹ = inversa da matriz de covariÃ¢ncia.

**Quando usar:**
- Features **correlacionadas** (ex: bytes_in ~ packets_in)
- DistribuiÃ§Ã£o elÃ­ptica
- DetecÃ§Ã£o de anomalias multivariadas

**LimitaÃ§Ã£o:** Precisa calcular matriz de covariÃ¢ncia; requer dados suficientes.

#### 3.6.4 Cosine Distance (DistÃ¢ncia do Cosseno)

**O que Ã©:** Mede o **Ã¢ngulo** entre vetores, ignorando magnitude.

**FÃ³rmula:**
```
similaridade = (A Â· B) / (||A|| Ã— ||B||)
distÃ¢ncia = 1 - similaridade
```

**Exemplo:** A=(3,4), B=(6,8) â†’ mesma direÃ§Ã£o â†’ distÃ¢ncia = 0

**Quando usar:**
- DireÃ§Ã£o mais importante que magnitude
- Dados de texto (TF-IDF)
- Alta dimensionalidade

#### 3.6.5 ComparaÃ§Ã£o Resumida

| DistÃ¢ncia | FÃ³rmula | SensÃ­vel a Escala | Considera CorrelaÃ§Ã£o | Melhor Para |
|-----------|---------|-------------------|---------------------|-------------|
| **Euclidiana** | âˆšÎ£(a-b)Â² | Sim | NÃ£o | Dados contÃ­nuos, mesma escala |
| **Manhattan** | Î£\|a-b\| | Sim | NÃ£o | Dados esparsos, robustez |
| **Mahalanobis** | âˆš[(x-Î¼)áµ€Î£â»Â¹(x-Î¼)] | NÃ£o | **Sim** | Dados correlacionados |
| **Cosseno** | 1 - cos(Î¸) | **NÃ£o** | NÃ£o | Texto, direÃ§Ã£o > magnitude |

#### 3.6.6 RecomendaÃ§Ã£o para IDS IoT

| CenÃ¡rio | DistÃ¢ncia Recomendada | Justificativa |
|---------|----------------------|---------------|
| Features normalizadas | **Euclidiana** | Simples, eficiente |
| Features correlacionadas | **Mahalanobis** | Captura correlaÃ§Ã£o |
| Features com outliers extremos | **Manhattan** | Mais robusta |
| Embedding de comportamento | **Cosseno** | PadrÃ£o > intensidade |

**Nota:** No TEDA, a escolha da distÃ¢ncia Ã© a **Ãºnica decisÃ£o** necessÃ¡ria (diferente de probabilidade que exige distribuiÃ§Ã£o + parÃ¢metros).

### 3.7 NormalizaÃ§Ã£o: Conceito e AplicaÃ§Ã£o no TEDA

#### 3.7.1 O que Ã© NormalizaÃ§Ã£o nas CiÃªncias Exatas

NormalizaÃ§Ã£o Ã© o processo de **transformar valores para uma escala comum**, permitindo comparaÃ§Ãµes justas entre grandezas diferentes.

```
Valor Normalizado = Valor Original / Fator de Escala
```

#### 3.7.2 Tipos Comuns de NormalizaÃ§Ã£o

| Tipo | FÃ³rmula | Propriedade | Exemplo |
|------|---------|-------------|---------|
| **Por Soma** | x/Î£x | Soma = 1 | Votos: 300/1000 = 30% |
| **Por MÃ¡ximo** | x/max(x) | MÃ¡ximo = 1 | Nota: 8/10 = 0.8 |
| **Min-Max** | (x-min)/(max-min) | Range [0,1] | Temp: (25-10)/(40-10) = 0.5 |
| **Z-Score** | (x-Î¼)/Ïƒ | MÃ©dia=0, Ïƒ=1 | Altura: (190-170)/10 = +2.0 |
| **L2 (UnitÃ¡rio)** | x/\|\|x\|\| | Norma = 1 | Vetor: (3,4)/5 = (0.6, 0.8) |

#### 3.7.3 Por que Normalizar?

| Problema sem normalizaÃ§Ã£o | SoluÃ§Ã£o com normalizaÃ§Ã£o |
|---------------------------|-------------------------|
| Escalas diferentes (km vs mm) | Valores comparÃ¡veis |
| NÃºmeros absolutos sem contexto | ProporÃ§Ãµes com significado |
| DominÃ¢ncia de features grandes | ContribuiÃ§Ã£o equilibrada |
| DifÃ­cil interpretar magnitudes | FÃ¡cil interpretar (0-1, %) |

#### 3.7.4 Por que Eccentricity Ã© Ï€ Normalizado

**Problema com Ï€ (proximidade acumulada):**

Ï€ Ã© um valor absoluto que depende de nÃºmero de pontos, escala e unidade:
```
Dados A: {1, 2, 3}       â†’  Ï€(2) = |2-1| + |2-3| = 2
Dados B: {10, 20, 30}    â†’  Ï€(20) = |20-10| + |20-30| = 20
```
O ponto "do meio" tem Ï€=2 em A e Ï€=20 em B, mas **ambos sÃ£o igualmente tÃ­picos**!

**SoluÃ§Ã£o: Normalizar Ï€ para obter Î¾**

A pergunta que queremos responder: "QuÃ£o excÃªntrico Ã© este ponto **RELATIVO** aos outros?"

```
Î¾_j = (2 Ã— Ï€_j) / Î£Ï€_i
```

| Componente | Significado |
|------------|-------------|
| `Ï€_j` | Proximidade acumulada do ponto j |
| `Î£Ï€_i` | Soma de TODAS as proximidades |
| `Ï€_j / Î£Ï€_i` | FraÃ§Ã£o da proximidade total que j representa |
| `2 Ã—` | Fator de correÃ§Ã£o (cada distÃ¢ncia contada 2x) |

#### 3.7.5 Exemplo NumÃ©rico Completo

```
Dados: {10, 12, 11, 25}

Proximidades acumuladas (Ï€):
Ï€(10) = |10-12| + |10-11| + |10-25| = 2 + 1 + 15 = 18
Ï€(12) = |12-10| + |12-11| + |12-25| = 2 + 1 + 13 = 16
Ï€(11) = |11-10| + |11-12| + |11-25| = 1 + 1 + 14 = 16
Ï€(25) = |25-10| + |25-12| + |25-11| = 15 + 13 + 14 = 42

Î£Ï€ = 18 + 16 + 16 + 42 = 92

Eccentricity (Î¾) â€” Ï€ normalizado:
Î¾(10) = (2 Ã— 18) / 92 = 0.39
Î¾(12) = (2 Ã— 16) / 92 = 0.35
Î¾(11) = (2 Ã— 16) / 92 = 0.35
Î¾(25) = (2 Ã— 42) / 92 = 0.91

Î£Î¾ = 0.39 + 0.35 + 0.35 + 0.91 = 2.0 âœ“
```

**InterpretaÃ§Ã£o:**
- Î¾(25) = 0.91 >> threshold (1/k = 0.25) â†’ **ANOMALIA**
- Î¾(10,11,12) â‰ˆ 0.35 ~ threshold â†’ pontos tÃ­picos

#### 3.7.6 O Fator 2

Por que multiplicar por 2 no numerador?

Cada distÃ¢ncia d(i,j) aparece **duas vezes** na soma total:
- Uma vez em Ï€_i (distÃ¢ncia de i para j)
- Uma vez em Ï€_j (distÃ¢ncia de j para i)

EntÃ£o: `Î£Ï€ = 2 Ã— (soma de todas as distÃ¢ncias Ãºnicas)`

O fator 2 no numerador garante que `Î£Î¾ = 2` (propriedade Ãºtil do framework).

#### 3.7.7 Vantagem da NormalizaÃ§Ã£o no TEDA

```
SEM normalizaÃ§Ã£o:        COM normalizaÃ§Ã£o (Î¾):
Ï€ = 42                   Î¾ = 0.91
"Ã‰ muito? Pouco?"        "91% de uma unidade"
Depende do contexto      Sempre comparÃ¡vel
Threshold arbitrÃ¡rio     Threshold = 1/k (universal)
```

**Resultado:** Î¾ âˆˆ (0,1) sempre, independente de quantos pontos, qual escala, ou qual unidade. O mesmo threshold (1/k) funciona para qualquer dataset.

---

## 4. Propriedades EstatÃ­sticas

### 4.1 Propriedades da Eccentricity e Typicality

- **Bounded:** Valores sempre entre 0 e 1
- **Normalizado:** Soma de Î¾ = 2, soma de Ï„ = k-2
- **Recursivo:** Pode ser atualizado incrementalmente sem recalcular tudo

### 4.2 Eccentricity e Typicality Normalizadas

#### 4.2.1 O Problema: Î¾ e Ï„ NÃ£o Somam 1

As versÃµes "cruas" tÃªm somas especÃ­ficas:
```
Î£Î¾ = 2        â† soma das eccentricidades de todos os pontos
Î£Ï„ = k - 2    â† soma das tipicalidades de todos os pontos
```

Isso dificulta a interpretaÃ§Ã£o como "distribuiÃ§Ã£o" â€” valores nÃ£o sÃ£o diretamente comparÃ¡veis a probabilidades.

#### 4.2.2 A SoluÃ§Ã£o: NormalizaÃ§Ã£o Adicional

O paper propÃµe (equaÃ§Ãµes 5 e 6):

```
Î¶_j = Î¾_j / 2           â†’ Î£Î¶ = 1   (eccentricity normalizada)
Ï„Ìƒ_j = Ï„_j / (k - 2)    â†’ Î£Ï„Ìƒ = 1   (typicality normalizada)
```

**Propriedades resultantes:**
- **0 < Î¶ < 1** para cada ponto
- **Î£Î¶ = 1** para todos os pontos
- **0 < Ï„Ìƒ < 1/(k-2)** para cada ponto
- **Î£Ï„Ìƒ = 1** para todos os pontos

#### 4.2.3 "Resembles Probability Distribution Function (PDF)"

**Por que a semelhanÃ§a?**

Uma PDF tem duas propriedades fundamentais:
1. **f(x) â‰¥ 0** para todo x (nÃ£o-negativa)
2. **âˆ«f(x)dx = 1** (integral/soma = 1)

A eccentricity normalizada Î¶ satisfaz ambas:
1. **0 < Î¶ < 1** (positiva e limitada)
2. **Î£Î¶ = 1** (soma = 1)

**PorÃ©m, hÃ¡ diferenÃ§as conceituais:**

| Aspecto | PDF (Probabilidade) | Î¶ (TEDA) |
|---------|---------------------|----------|
| SuposiÃ§Ã£o prÃ©via | DistribuiÃ§Ã£o conhecida (Gaussiana, etc.) | Nenhuma |
| IndependÃªncia | Amostras devem ser independentes | Usa dependÃªncia espacial |
| O que mede | FrequÃªncia esperada de ocorrÃªncia | DistÃ¢ncia relativa aos outros |
| InterpretaÃ§Ã£o | "Chance de um novo dado cair aqui" | "QuÃ£o excÃªntrico Ã© este dado" |
| Base | Modelo assumido | Dados observados apenas |

**CitaÃ§Ã£o do paper (linhas 356-362):**
> "Normalised eccentricity and typicality resemble probability distribution function (pdf) in that they sum to 1, but they are different as they do not require the prior assumptions that are a must for the probability theory and they represent both the **spatial distribution pattern** and the **frequency of occurrence** of a data sample."

#### 4.2.4 Exemplo NumÃ©rico: Î¾ vs Î¶

```
Dados: {2, 3, 3, 10} (k=4 pontos)

Proximidades acumuladas (Ï€):
Ï€â‚ = |2-2| + |2-3| + |2-3| + |2-10| = 0 + 1 + 1 + 8 = 10
Ï€â‚‚ = |3-2| + |3-3| + |3-3| + |3-10| = 1 + 0 + 0 + 7 = 8
Ï€â‚ƒ = |3-2| + |3-3| + |3-3| + |3-10| = 1 + 0 + 0 + 7 = 8
Ï€â‚„ = |10-2| + |10-3| + |10-3| + |10-10| = 8 + 7 + 7 + 0 = 22

Î£Ï€ = 10 + 8 + 8 + 22 = 48

Eccentricity crua (Î¾):
Î¾â‚ = (2 Ã— 10) / 48 = 0.417
Î¾â‚‚ = (2 Ã— 8) / 48 = 0.333
Î¾â‚ƒ = (2 Ã— 8) / 48 = 0.333
Î¾â‚„ = (2 Ã— 22) / 48 = 0.917

Î£Î¾ = 0.417 + 0.333 + 0.333 + 0.917 = 2.0 âœ“

Eccentricity normalizada (Î¶):
Î¶â‚ = Î¾â‚ / 2 = 0.208
Î¶â‚‚ = Î¾â‚‚ / 2 = 0.167
Î¶â‚ƒ = Î¾â‚ƒ / 2 = 0.167
Î¶â‚„ = Î¾â‚„ / 2 = 0.458

Î£Î¶ = 0.208 + 0.167 + 0.167 + 0.458 = 1.0 âœ“ (como PDF!)
```

**InterpretaÃ§Ã£o:**
- Se fosse probabilidade uniforme: cada ponto teria 25% (1/4)
- Î¶â‚„ = 0.458 â†’ o ponto 10 "concentra" **45.8%** da excentricidade total
- Isso mostra que ponto 10 Ã© **muito mais excÃªntrico** que os outros

#### 4.2.5 DiferenÃ§a Conceitual Fundamental

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF responde: "Qual a CHANCE de um novo dado cair aqui?"   â”‚
â”‚   â†’ Baseado em MODELO assumido (Gaussiana, etc.)           â”‚
â”‚                                                             â”‚
â”‚ Î¶ responde: "QuÃ£o EXCÃŠNTRICO Ã© este dado especÃ­fico?"      â”‚
â”‚   â†’ Baseado APENAS nas DISTÃ‚NCIAS observadas               â”‚
â”‚                                                             â”‚
â”‚ Ambos somam 1, mas medem coisas diferentes:                â”‚
â”‚   - PDF: distribuiÃ§Ã£o ESPERADA (modelo teÃ³rico)            â”‚
â”‚   - Î¶: distribuiÃ§Ã£o OBSERVADA (dados reais)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2.6 Por Que Isso Ã‰ Ãštil?

A semelhanÃ§a com PDF permite:

1. **InterpretaÃ§Ã£o intuitiva**: "Este ponto tem 45% da excentricidade" Ã© fÃ¡cil de entender
2. **ComparaÃ§Ã£o justa**: Valores entre 0-1 permitem comparar datasets diferentes
3. **Threshold natural**: 1/k Ã© o valor "esperado" se todos fossem igualmente excÃªntricos
4. **Compatibilidade**: Pode-se usar Î¶ em fÃ³rmulas que esperam valores tipo-probabilidade

**Sem as limitaÃ§Ãµes da probabilidade:**
- Funciona com k â‰¥ 3 amostras (nÃ£o precisa de infinitas)
- Usa a estrutura espacial (nÃ£o assume independÃªncia)
- NÃ£o precisa assumir distribuiÃ§Ã£o prÃ©via

#### 4.2.7 Analogia com Histograma (linhas 379-382)

O paper tambÃ©m menciona que tipicalidade Ã© anÃ¡loga a histogramas:

> "The typicality can also be seen as an analogue to the histograms of distributions, but it is in a **closed analytical form** and does take into account the **mutual influence** of the neighbouring data samples/observations."

| Aspecto | Histograma | Tipicalidade (Ï„) |
|---------|------------|------------------|
| Forma | Discreto (bins) | AnalÃ­tico (fÃ³rmula) |
| Depende de | Escolha de bins (arbitrÃ¡rio) | Apenas dados |
| Considera vizinhanÃ§a | NÃ£o (conta frequÃªncia apenas) | Sim (distÃ¢ncias) |

---

## 5. O Framework TEDA

### 5.1 Algoritmo (Algorithm 1 do paper)

```
Inicializar: k=1; xâ‚; XÂ¹=||xâ‚||Â²; mÂ¹=xâ‚; Ï€â‚Â¹=0

ENQUANTO dados disponÃ­veis:
    1. Ler prÃ³ximo ponto x_k (k := k+1)
    2. Atualizar:
       a. m^k (mÃ©dia)
       b. X^k (produto escalar)
    3. Para 1 â‰¤ j â‰¤ k, computar Ï€_j^k
    4. Atualizar Î£Ï€
    5. Para 1 â‰¤ j â‰¤ k, computar:
       a. Î¾_j^k (eccentricity)
       b. Ï„_j^k (typicality)
       c. Î¶_j^k (eccentricity normalizada)
       d. t_j^k (typicality normalizada)
FIM ENQUANTO
```

### 5.2 FÃ³rmulas de AtualizaÃ§Ã£o Recursiva (Euclidean)

#### 5.2.1 FÃ³rmulas IntermediÃ¡rias

```
Ï€_j^k = k Ã— ||x_j - Î¼^k||Â² + X^k - ||Î¼^k||Â²

Î¼^k = ((k-1)/k) Ã— Î¼^(k-1) + (1/k) Ã— x_k       (mÃ©dia recursiva)

X^k = ((k-1)/k) Ã— X^(k-1) + (1/k) Ã— ||x_k||Â²  (produto escalar recursivo)

Î£Ï€^k = Î£Ï€^(k-1) + 2Ã—Ï€_k^k                      (soma recursiva)
```

#### 5.2.2 FÃ³rmula Direta para Eccentricity

O paper demonstra que, para distÃ¢ncia Euclidiana quadrada, a eccentricity pode ser calculada **diretamente** usando apenas mÃ©dia e variÃ¢ncia:

```
Î¾_j^k = 1/k + ||x_j - Î¼^k||Â² / (k Ã— ÏƒÂ²)
```

Onde:
- `k` = nÃºmero de amostras
- `Î¼^k` = mÃ©dia de todas as amostras
- `ÏƒÂ²` = variÃ¢ncia (dispersÃ£o mÃ©dia)
- `||x_j - Î¼^k||Â²` = distÃ¢ncia quadrada do ponto j ao centro

**InterpretaÃ§Ã£o dos componentes:**

| Componente | Significado |
|------------|-------------|
| `1/k` | Eccentricidade mÃ­nima (se ponto estiver exatamente na mÃ©dia) |
| `(x - Î¼)Â²` | QuÃ£o longe o ponto estÃ¡ do centro |
| `k Ã— ÏƒÂ²` | Fator de escala (normaliza pela dispersÃ£o total) |

#### 5.2.3 DerivaÃ§Ã£o MatemÃ¡tica: Da DefiniÃ§Ã£o Original Ã  FÃ³rmula Recursiva

**Problema:** A definiÃ§Ã£o original requer O(nÂ²) cÃ¡lculos de distÃ¢ncia. Como chegar Ã  fÃ³rmula O(n)?

##### Passo 1: DefiniÃ§Ã£o Original

Proximidade acumulada do ponto j:
```
Ï€_j = Î£áµ¢ ||x_j - x_i||Â²    (soma das distÃ¢ncias quadradas a todos os outros pontos)
```

##### Passo 2: Identidade AlgÃ©brica Fundamental (Teorema de Huygens-Steiner)

Existe uma identidade clÃ¡ssica que relaciona distÃ¢ncias pairwise com distÃ¢ncia Ã  mÃ©dia:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Î£áµ¢ ||x_j - x_i||Â² = kÂ·||x_j - Î¼||Â² + kÂ·ÏƒÂ²                     â”‚
â”‚                                                                â”‚
â”‚  "Soma das distÃ¢ncias a todos" = "k Ã— (distÃ¢ncia Ã  mÃ©dia)Â²"   â”‚
â”‚                                   + "k Ã— variÃ¢ncia"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Nomes desta identidade em diferentes Ã¡reas:**

| Ãrea | Nome | Forma ClÃ¡ssica |
|------|------|----------------|
| **FÃ­sica/MecÃ¢nica** | Teorema de Steiner (Eixos Paralelos) | I = I_cm + mÂ·dÂ² |
| **EstatÃ­stica** | FÃ³rmula de KÃ¶nig-Huygens | Var(X) = E[XÂ²] - E[X]Â² |
| **ML/Data Science** | DecomposiÃ§Ã£o do CentrÃ³ide | Î£d_ijÂ² = kÂ·d_Î¼Â² + kÂ·ÏƒÂ² |

**Teorema de Huygens-Steiner (FÃ­sica):**
> "O momento de inÃ©rcia em relaÃ§Ã£o a qualquer eixo Ã© igual ao momento de inÃ©rcia em relaÃ§Ã£o ao centro de massa, mais a massa vezes a distÃ¢ncia ao quadrado."

Na versÃ£o para dados, a "inÃ©rcia" Ã© a soma das distÃ¢ncias quadradas, e o "centro de massa" Ã© a mÃ©dia Î¼.

**FÃ³rmula de KÃ¶nig-Huygens (EstatÃ­stica):**
```
Var(X) = E[XÂ²] - (E[X])Â²
ÏƒÂ² = X - ||Î¼||Â²
```

Esta Ã© exatamente a relaÃ§Ã£o usada no Passo 4 para expressar ÏƒÂ² em termos de X e Î¼.

##### Passo 3: Prova da Identidade

Reescrevendo cada distÃ¢ncia em termos de desvios da mÃ©dia:
```
||x_j - x_i||Â² = ||(x_j - Î¼) - (x_i - Î¼)||Â²
```

Expandindo o quadrado:
```
= ||x_j - Î¼||Â² - 2(x_j - Î¼)Â·(x_i - Î¼) + ||x_i - Î¼||Â²
```

Somando sobre todos os i = 1, 2, ..., k:
```
Î£áµ¢ ||x_j - x_i||Â² = Î£áµ¢ ||x_j - Î¼||Â² - 2(x_j - Î¼)Â·Î£áµ¢(x_i - Î¼) + Î£áµ¢ ||x_i - Î¼||Â²
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      kÂ·||x_j - Î¼||Â²           = 0                   kÂ·ÏƒÂ²
```

**O termo do meio Ã© ZERO!** Por quÃª?
```
Î£áµ¢ (x_i - Î¼) = Î£áµ¢ x_i - kÂ·Î¼ = kÂ·Î¼ - kÂ·Î¼ = 0
```
(A soma dos desvios em relaÃ§Ã£o Ã  mÃ©dia Ã© sempre zero â€” propriedade fundamental da mÃ©dia!)

**Resultado:**
```
Ï€_j = kÂ·||x_j - Î¼||Â² + kÂ·ÏƒÂ²
```

##### Passo 4: O Papel do Produto Escalar X

Precisamos expressar ÏƒÂ² de forma que possa ser atualizada incrementalmente.

A **variÃ¢ncia** tem a fÃ³rmula:
```
ÏƒÂ² = E[||x||Â²] - ||E[x]||Â² = E[||x||Â²] - ||Î¼||Â²
```

O paper define **X** como a mÃ©dia dos quadrados das normas:
```
X = (1/k) Â· Î£áµ¢ ||x_i||Â²    (mÃ©dia de ||x||Â²)
```

Portanto:
```
ÏƒÂ² = X - ||Î¼||Â²
```

| EstatÃ­stica | O que guarda | AtualizaÃ§Ã£o |
|-------------|--------------|-------------|
| **Î¼** (mÃ©dia) | Centro de massa dos dados | O(1) por novo ponto |
| **X** (mÃ©dia de \|\|x\|\|Â²) | "Energia" mÃ©dia dos dados | O(1) por novo ponto |
| **ÏƒÂ² = X - \|\|Î¼\|\|Â²** | VariÃ¢ncia (dispersÃ£o) | Calculado de Î¼ e X |

##### Passo 5: SubstituiÃ§Ã£o

Substituindo ÏƒÂ² = X - ||Î¼||Â² na fÃ³rmula de Ï€:
```
Ï€_j = kÂ·||x_j - Î¼||Â² + kÂ·ÏƒÂ²
    = kÂ·||x_j - Î¼||Â² + kÂ·(X - ||Î¼||Â²)
    = kÂ·||x_j - Î¼||Â² + kÂ·X - kÂ·||Î¼||Â²
```

##### Passo 6: Da FÃ³rmula de Ï€ para Î¾

A eccentricity Ã© definida como:
```
Î¾_j = 2Ï€_j / Î£Ï€
```

ApÃ³s simplificaÃ§Ã£o algÃ©brica (referenciada em [3],[4] do paper), obtÃ©m-se:
```
Î¾_j = 1/k + ||x_j - Î¼||Â² / (kÂ·ÏƒÂ²)
```

##### Resumo da DerivaÃ§Ã£o

```
PASSO 1: DefiniÃ§Ã£o
  Ï€_j = Î£áµ¢ ||x_j - x_i||Â²               (soma de todas as distÃ¢ncias)

PASSO 2: Identidade algÃ©brica
  Î£áµ¢ ||x_j - x_i||Â² = kÂ·||x_j - Î¼||Â² + kÂ·ÏƒÂ²

PASSO 3: Expressar variÃ¢ncia
  ÏƒÂ² = X - ||Î¼||Â²                       (X = mÃ©dia de ||x||Â²)

PASSO 4: Substituir
  Ï€_j = kÂ·||x_j - Î¼||Â² + kÂ·(X - ||Î¼||Â²)

PASSO 5: Eccentricity simplificada
  Î¾_j = 2Ï€_j / Î£Ï€  â†’  simplifica para  â†’  Î¾_j = 1/k + ||x_j - Î¼||Â²/(kÂ·ÏƒÂ²)

RESULTADO:
  Î¾ depende APENAS de: Î¼ (mÃ©dia), ÏƒÂ² (variÃ¢ncia), x_j (ponto atual)
  Complexidade: O(1) para calcular Î¾ de um novo ponto!
```

##### VisualizaÃ§Ã£o: O(nÂ²) vs O(n)

```
ORIGINAL (O(nÂ²)):                    RECURSIVA (O(n)):
    xâ‚ â†â†’ xâ‚‚                                  xâ‚
    xâ‚ â†â†’ xâ‚ƒ     Calcular TODAS                â†˜
    xâ‚ â†â†’ xâ‚„     as distÃ¢ncias          xâ‚‚ â”€â”€â†’ Î¼ â†â”€â”€ xâ‚ƒ
    xâ‚‚ â†â†’ xâ‚ƒ     pairwise                      â†—
    xâ‚‚ â†â†’ xâ‚„                                  xâ‚„
    xâ‚ƒ â†â†’ xâ‚„
                                       Cada ponto sÃ³ calcula
    6 distÃ¢ncias para 4 pontos         distÃ¢ncia Ã  MÃ‰DIA (1 cada)
    n(n-1)/2 no geral                  + ÏƒÂ² vem "de brinde" via X
```

#### 5.2.4 Aplicabilidade a Outras MÃ©tricas de DistÃ¢ncia

A derivaÃ§Ã£o acima usa **propriedades especÃ­ficas da distÃ¢ncia Euclidiana quadrada**:
- ExpansÃ£o do quadrado: `||a-b||Â² = ||a||Â² - 2aÂ·b + ||b||Â²`
- Cancelamento do termo cruzado via propriedade da mÃ©dia

**Para outras mÃ©tricas:**

| MÃ©trica | FÃ³rmula Recursiva? | ReferÃªncia no Paper |
|---------|-------------------|---------------------|
| **EuclidianaÂ²** | âœ… Î¾ = 1/k + \|\|x-Î¼\|\|Â²/(kÂ·ÏƒÂ²) | [3], [4] |
| **MahalanobisÂ²** | âœ… Similar, com matriz Î£â»Â¹ | [6] |
| **Cosseno** | âœ… Usa produto interno normalizado | [5] |
| **Manhattan (L1)** | âŒ NÃ£o tem forma fechada simples | Usa O(nÂ²) ou aproximaÃ§Ãµes |

**CitaÃ§Ã£o do paper (linhas 390-395):**
> "It can also be proven that both eccentricity and typicality can be calculated recursively by updating only the global or local mean, Î¼ and scalar product, X for the cases when **Euclidean square distance** [3],[4] is used and similarly if **cosine** [5] or **Mahalonobis square distance** [6] are used."

**Nota:** Para Manhattan, a propriedade algÃ©brica `||a-b||Â² = ||a||Â² - 2aÂ·b + ||b||Â²` nÃ£o se aplica (pois |a-b| â‰  |a|Â² - 2ab + |b|Â²), entÃ£o a simplificaÃ§Ã£o nÃ£o Ã© possÃ­vel.

#### 5.2.5 ImplementaÃ§Ã£o Python (Lab de Clustering)

CÃ³digo do laboratÃ³rio que implementa a fÃ³rmula recursiva:

```python
def calculate_eccentricity_batch(X):
    """
    Calcula eccentricidade para cada ponto em um batch.
    Eccentricidade alta = ponto Ã© outlier/diferente
    """
    n = len(X)
    mean = np.mean(X, axis=0)                           # Î¼

    # DistÃ¢ncia de cada ponto Ã  mÃ©dia
    distances_to_mean = np.sum((X - mean) ** 2, axis=1) # ||x_j - Î¼||Â²

    # VariÃ¢ncia total
    variance = np.mean(distances_to_mean)               # ÏƒÂ²

    # Eccentricidade: Î¾ = 1/k + (x - Î¼)Â² / (k Ã— ÏƒÂ²)
    if variance > 0:
        eccentricity = (1/n) + (distances_to_mean / (n * variance))
    else:
        eccentricity = np.ones(n) / n  # Caso degenerado

    return eccentricity
```

**Mapeamento cÃ³digo â†’ fÃ³rmula:**

| Paper | CÃ³digo | Significado |
|-------|--------|-------------|
| `k` | `n` | NÃºmero de pontos |
| `Î¼^k` | `mean` | MÃ©dia do dataset |
| `\|\|x_j - Î¼\|\|Â²` | `distances_to_mean` | DistÃ¢ncia quadrada ao centro |
| `ÏƒÂ²` | `variance` | VariÃ¢ncia (dispersÃ£o mÃ©dia) |
| `1/k` | `1/n` | Termo base (eccentricidade mÃ­nima) |

#### 5.2.6 VerificaÃ§Ã£o NumÃ©rica

Usando o exemplo {2, 3, 3, 10}:

```python
import numpy as np

X = np.array([[2], [3], [3], [10]])
n = 4
mean = np.mean(X)  # = 4.5

distances_to_mean = (X.flatten() - mean) ** 2
# = [6.25, 2.25, 2.25, 30.25]

variance = np.mean(distances_to_mean)  # = 10.25

# FÃ³rmula recursiva: Î¾ = 1/k + (x - Î¼)Â² / (k Ã— ÏƒÂ²)
xi = (1/n) + (distances_to_mean / (n * variance))
# xi = 0.25 + [6.25, 2.25, 2.25, 30.25] / 41.0
# xi = 0.25 + [0.152, 0.055, 0.055, 0.738]
# xi = [0.402, 0.305, 0.305, 0.988]

print(f"Î¾ = {xi}")       # [0.402, 0.305, 0.305, 0.988]
print(f"Î£Î¾ = {sum(xi)}") # 2.0 âœ“
```

**Resultado:**
```
Î¾(2)  = 0.402  (tÃ­pico, < 1)
Î¾(3)  = 0.305  (tÃ­pico, < 1)
Î¾(3)  = 0.305  (tÃ­pico, < 1)
Î¾(10) = 0.988  (ANOMALIA! >> threshold 1/k = 0.25)
```

#### 5.2.7 Por Que Isso Ã‰ Importante para Streaming

A fÃ³rmula recursiva permite **atualizaÃ§Ã£o incremental**:

```python
# Streaming: atualizaÃ§Ã£o O(1) por novo ponto
def update_streaming(new_point, k, old_mean, old_variance):
    # Atualiza mÃ©dia incrementalmente
    new_mean = old_mean + (new_point - old_mean) / (k + 1)

    # Atualiza variÃ¢ncia incrementalmente (Welford's algorithm)
    # ... (detalhes no MicroTEDAclus)

    # Calcula Î¾ do novo ponto em O(1)
    dist_sq = (new_point - new_mean) ** 2
    xi_new = 1/(k+1) + dist_sq / ((k+1) * new_variance)

    return xi_new, new_mean, new_variance
```

**ImplicaÃ§Ã£o:** TEDA pode processar **milhÃµes de pontos em streaming** sem armazenar histÃ³rico, apenas mantendo Î¼ e ÏƒÂ².

### 5.3 Vantagens do Approach

1. **Sem prior assumptions:** NÃ£o assume distribuiÃ§Ã£o
2. **Sem kernels:** NÃ£o precisa escolher funÃ§Ã£o/bandwidth
3. **Recursivo:** Computacionalmente eficiente para streams
4. **Funciona com poucos dados:** k â‰¥ 3 Ã© suficiente
5. **Usa dependÃªncia mÃºtua:** Captura estrutura espacial dos dados
6. **Closed-form:** ExpressÃµes analÃ­ticas fechadas

---

## 6. AplicaÃ§Ãµes Demonstradas

| AplicaÃ§Ã£o | Dataset | Resultados |
|-----------|---------|------------|
| Temperatura | {20, 12, 10} Â°C (exemplo didÃ¡tico) | 20Â°C identificado como excÃªntrico (Î¾=0.45 > 1/3) |
| Temperatura | {20, 12, 10, 17} Â°C | PadrÃ£o rebalanceado, todos mais tÃ­picos |
| PrecipitaÃ§Ã£o | Bristol, UK, Jan 2014 (14 dias) | 20.2mm no Ano Novo identificado como atÃ­pico |

---

## 7. SeÃ§Ãµes do Paper (Detalhado)

### 7.1 Anomaly Detection (SeÃ§Ã£o 4)

**PrincÃ­pio:** DetecÃ§Ã£o baseada em eccentricity â€” pontos com Î¾ alto sÃ£o candidatos a anomalias.

**Regra bÃ¡sica:**
```
SE Î¶ > 1/k  â†’  Ponto Ã© ANOMALIA SUSPEITA
SE Ï„ < 1/k  â†’  Ponto Ã© ATÃPICO
```

**Vantagens sobre probabilidade:**

| Aspecto | Probabilidade | TEDA (Eccentricity) |
|---------|---------------|---------------------|
| Exemplo {20, 12, 10}Â°C | P(cada) = 1/3 (iguais!) | Î¾(20) = 0.45 >> Î¾(12) = 0.25 |
| Precisa de distribuiÃ§Ã£o | Sim (Gaussiana, etc.) | NÃ£o |
| Quantidade de dados | Muitos (para convergir) | Funciona com k â‰¥ 3 |
| Forma analÃ­tica | Aproximada (KDE, histograma) | Exata (fÃ³rmula fechada) |

**Grau de severidade:**
- NÃ£o Ã© binÃ¡rio (anomalia/normal)
- Î¶ indica **quÃ£o anÃ´malo** Ã© o ponto
- "How bigger Î¶ is in comparison with 1/k" (linha 1504)

**AplicaÃ§Ãµes citadas:**
- Processamento de imagens e vÃ­deo
- DetecÃ§Ã£o de falhas
- Modelagem de comportamento de usuÃ¡rios
- Eventos extremos (clima, terremotos, terrorismo)

---

### 7.2 Clustering e Data Clouds (SeÃ§Ã£o 5) â€” ESSENCIAL PARA MicroTEDAclus

#### 7.2.1 Conceito de "Data Cloud"

**DefiniÃ§Ã£o (linhas 1514-1517):**
> "The term 'data cloud' was introduced in the so called AnYa framework [8] and **differs from clusters** by the fact that data clouds have **no specific shape, parameters, and boundaries**."

| CaracterÃ­stica | Cluster Tradicional | Data Cloud (TEDA) |
|----------------|--------------------|--------------------|
| Forma | EsfÃ©rica, elÃ­ptica, etc. | Livre (sem forma fixa) |
| ParÃ¢metros | k (nÃºmero), raio, etc. | Nenhum prÃ©-definido |
| Fronteiras | Definidas (hard/soft) | FlexÃ­veis |
| Centro | CentrÃ³ide calculado | Ponto com maior Ï„ (focal point) |

#### 7.2.2 FormaÃ§Ã£o de Data Clouds

**Algoritmo conceitual (linhas 1520-1536):**

```
1. Encontrar ponto com MAIOR Ï„ â†’ Primeiro "focal point" (protÃ³tipo)
2. Definir "zona de influÃªncia" (raio) ao redor do focal point
3. Para pontos FORA da zona de influÃªncia:
   - SE Ï„ > 1/k â†’ Candidato a novo focal point
   - Selecionar o de maior Ï„ como prÃ³ximo protÃ³tipo
4. Repetir atÃ© nÃ£o haver mais candidatos
```

**CritÃ©rio para ser protÃ³tipo:** Ï„ > 1/k (tipicalidade acima da mÃ©dia)

#### 7.2.3 Tipicalidade Local vs Global

**Global:** Calculada sobre TODOS os k pontos
```
Ï„_global = tipicalidade em relaÃ§Ã£o ao dataset inteiro
```

**Local:** Calculada sobre pontos de UMA data cloud especÃ­fica
```
Ï„_local = tipicalidade em relaÃ§Ã£o apenas aos membros do cluster
```

Isso permite extrair distribuiÃ§Ãµes separadas por cluster:
```
Data cloud "azul": Î¾_blue, Ï„_blue
Data cloud "vermelha": Î¾_red, Ï„_red
```

#### 7.2.4 EficiÃªncia em Streaming (MemÃ³ria)

**O que manter por data cloud (linhas 1551-1554):**
```
Para cada data cloud i*, guardar apenas:
- x_i*  (ponto focal/protÃ³tipo)
- Î¼_i*  (mÃ©dia local)
- X_i*  (produto escalar local)
- Î£Ï€    (soma das proximidades)
```

**NÃƒO precisa guardar todos os pontos!** Apenas as estatÃ­sticas agregadas.

#### 7.2.5 Aspectos DinÃ¢micos (linhas 1581-1584)

> "An important aspect is the **dynamic nature** of the data streams and their **order dependency**. One can chose to have a **forgetting factor** or mechanism to introduce the importance of the time instances when a particular data sample was read."

**ImplicaÃ§Ãµes:**
- Data clouds podem **evoluir** com o tempo
- Pontos antigos podem ter peso menor (forgetting factor)
- Ordem de chegada importa (streaming nÃ£o Ã© batch)

#### 7.2.6 Por que "Fora da Zona + Ï„ > 1/k" para Novo ProtÃ³tipo?

**O Problema:** Como formar MÃšLTIPLOS clusters sem definir k (nÃºmero de clusters)?

Se apenas pegÃ¡ssemos o ponto com maior Ï„, terÃ­amos um Ãºnico cluster. Precisamos de critÃ©rio para identificar centros de OUTROS grupos.

**A SoluÃ§Ã£o: Duas CondiÃ§Ãµes SimultÃ¢neas**

```
Para ser protÃ³tipo de um NOVO cluster, o ponto deve:
1. Estar FORA da zona de influÃªncia â†’ "Longe o suficiente do primeiro protÃ³tipo"
2. Ter Ï„ > 1/k                      â†’ "TÃ­pico o suficiente para ser centro"
```

**Por que as duas condiÃ§Ãµes?**

| CondiÃ§Ã£o | O que garante | Sem ela... |
|----------|---------------|------------|
| Fora da zona | Clusters sÃ£o **separados** | ProtÃ³tipos muito prÃ³ximos = mesmo cluster |
| Ï„ > 1/k | ProtÃ³tipo Ã© **representativo** | Outlier isolado viraria centro (ruim!) |

**Exemplo visual:**
```
Dados: â— â— â— â—    â—‹ â—‹ â—‹ â—‹ â—‹        âœ— (outlier)
       grupo A       grupo B

Passo 1: Maior Ï„ global â†’ â— central (protÃ³tipo de A)
Passo 2: Zona de influÃªncia ao redor do â— central
Passo 3: Fora da zona, quem tem Ï„ > 1/k?
         â†’ â—‹ central Ã© candidato (Ï„ alto, fora da zona) âœ“
         â†’ âœ— outlier NÃƒO Ã© (Ï„ < 1/k, muito excÃªntrico) âœ—
Passo 4: â—‹ com maior Ï„ â†’ protÃ³tipo de B
```

**âš ï¸ ATENÃ‡ÃƒO:** O paper NÃƒO define como calcular a "zona de influÃªncia" â€” Ã© deixado como escolha de design. Ver seÃ§Ã£o 8.3 para detalhes desta lacuna.

#### 7.2.7 Por que Guardar Apenas {Î¼, X, Î£Ï€} por Cluster?

**O Problema: Streaming com MilhÃµes de Pontos**

```
CenÃ¡rio IDS IoT:
- 10.000 pacotes/segundo
- 1 hora = 36 milhÃµes de pontos
- Guardar todos = ğŸ’¥ MEMÃ“RIA EXPLODE
```

**A SoluÃ§Ã£o: EstatÃ­sticas Suficientes**

GraÃ§as ao **Teorema de Huygens-Steiner**, a fÃ³rmula recursiva sÃ³ precisa de:

```
Î¾_j = 1/k + ||x_j - Î¼||Â² / (k Ã— ÏƒÂ²)

onde ÏƒÂ² = X - ||Î¼||Â²
```

**Para calcular Î¾ de um NOVO ponto, basta ter:**
- **Î¼** (mÃ©dia) â€” centro do cluster
- **X** (mÃ©dia de ||x||Â²) â€” para calcular ÏƒÂ²
- **k** (contador) â€” quantos pontos jÃ¡ processados
- **Î£Ï€** (soma de proximidades) â€” para eccentricity global

**ComparaÃ§Ã£o de MemÃ³ria:**

| Abordagem | MemÃ³ria (1M pontos, 10 clusters) |
|-----------|----------------------------------|
| Batch (todos os pontos) | O(1.000.000 Ã— n_features) â‰ˆ GB |
| Streaming (estatÃ­sticas) | O(10 Ã— 4) = 40 valores â‰ˆ bytes |

**Por que funciona?** O Teorema de Huygens-Steiner "comprime" toda a informaÃ§Ã£o sobre distÃ¢ncias pairwise em apenas Î¼ e X:
```
Î£áµ¢ ||x_j - x_i||Â² = kÂ·||x_j - Î¼||Â² + kÂ·ÏƒÂ²
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    CalculÃ¡vel sÃ³ com Î¼, X, k
```

#### 7.2.8 O Significado de 1/k como Threshold

**O que 1/k representa?**

Para mÃ©tricas normalizadas (Î¶ e Ï„Ìƒ) que somam 1:

```
Î£Î¶ = 1  (soma das eccentricidades normalizadas)
Î£Ï„Ìƒ = 1  (soma das tipicalidades normalizadas)

Se TODOS os k pontos fossem igualmente tÃ­picos/excÃªntricos:
   Cada um teria exatamente 1/k do total

1/k = "cota justa" = valor esperado sob distribuiÃ§Ã£o uniforme
```

**A LÃ³gica do Threshold:**

| ComparaÃ§Ã£o | InterpretaÃ§Ã£o | ConclusÃ£o |
|------------|---------------|-----------|
| Î¶ > 1/k | Mais excÃªntrico que a mÃ©dia | Candidato a **anomalia** |
| Î¶ < 1/k | Menos excÃªntrico que a mÃ©dia | Ponto **normal** |
| Ï„Ìƒ > 1/k | Mais tÃ­pico que a mÃ©dia | Candidato a **protÃ³tipo** |
| Ï„Ìƒ < 1/k | Menos tÃ­pico que a mÃ©dia | NÃ£o serve como centro |

**Exemplo numÃ©rico:**
```
Dados: {2, 3, 3, 10}  (k = 4)
Threshold: 1/k = 0.25

Î¶(2)  = 0.208 < 0.25 â†’ tÃ­pico
Î¶(3)  = 0.167 < 0.25 â†’ tÃ­pico
Î¶(3)  = 0.167 < 0.25 â†’ tÃ­pico
Î¶(10) = 0.458 > 0.25 â†’ ANÃ”MALO! (quase 2Ã— a cota justa)
```

**ConexÃ£o com a fÃ³rmula recursiva:**
```
Î¾ = 1/k + ||x - Î¼||Â² / (k Ã— ÏƒÂ²)
    â””â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   base    penalidade por distÃ¢ncia

- Ponto na mÃ©dia (x = Î¼): Î¾ = 1/k (mÃ­nimo possÃ­vel)
- Ponto longe da mÃ©dia: Î¾ >> 1/k (anÃ´malo)
```

**Por que 1/k Ã© elegante:**
1. **Adapta-se automaticamente** ao tamanho do dataset
2. **NÃ£o requer cÃ¡lculo adicional** â€” deriva direto de k
3. **InterpretaÃ§Ã£o clara** â€” "acima/abaixo da mÃ©dia esperada"

#### 7.2.9 ConexÃ£o com MicroTEDAclus

| TEDA (Angelov 2014) | MicroTEDAclus (Maia 2020) |
|---------------------|---------------------------|
| Data cloud = conjunto de pontos | Micro-cluster = resumo estatÃ­stico |
| Focal point = ponto com maior Ï„ | CentrÃ³ide do micro-cluster |
| Zona de influÃªncia (raio) | Merge/split baseado em tipicalidade |
| Tipicalidade local | "Mixture of typicalities" |
| Streaming bÃ¡sico | Streaming com concept drift |
| Guardar {Î¼, X, Î£Ï€} | Mesma ideia, mais refinada |
| Threshold 1/k | Chebyshev test adaptativo |

**Esta seÃ§Ã£o Ã© a PONTE CONCEITUAL para entender MicroTEDAclus!**

---

### 7.3 Classification (SeÃ§Ã£o 6)
- Usa valores locais de Î¾, Ï„ por classe
- Zero, first ou higher order classifiers

### 7.4 Prediction and Control (SeÃ§Ã£o 7)
- Multi-model principle com sub-modelos locais simples
- DecomposiÃ§Ã£o do espaÃ§o de dados em regiÃµes locais

---

## 8. LimitaÃ§Ãµes Identificadas

### 8.1 Reconhecida pelo Autor

> "TEDA can work efficiently with any data **except pure random processes**" (linhas 99-100)

**Por que TEDA falha em processos puramente aleatÃ³rios:**
- TEDA busca **dependÃªncia espacial** que **nÃ£o existe** em dados aleatÃ³rios puros
- Para dado justo, cada face tem mesma probabilidade â€” nÃ£o hÃ¡ padrÃ£o espacial
- TEDA converge para tipicalidade igual para todos, igual Ã  probabilidade
- **Mais trabalho computacional, mesmo resultado**

**Para processos aleatÃ³rios puros (dados, moedas), probabilidade clÃ¡ssica Ã© melhor.**

### 8.2 Outras LimitaÃ§Ãµes (implÃ­citas)
- Paper apresenta apenas exemplos didÃ¡ticos pequenos
- NÃ£o hÃ¡ comparaÃ§Ã£o quantitativa com outros mÃ©todos
- Escolha da mÃ©trica de distÃ¢ncia ainda Ã© necessÃ¡ria

### 8.3 Zona de InfluÃªncia NÃƒO Definida (Lacuna Importante)

**O que o paper diz (linhas 1526-1527):**
> "For example, a zone of influence/radius **can be defined**..."

A linguagem "**can be defined**" indica que Ã© uma **escolha de design**, nÃ£o uma fÃ³rmula fixa.

**O que isso significa:**

| Aspecto | O que o paper DIZ | O que NÃƒO diz |
|---------|-------------------|---------------|
| Conceito | "zona ao redor do protÃ³tipo" | FÃ³rmula especÃ­fica |
| Uso | "pontos fora sÃ£o candidatos a novos protÃ³tipos" | Como calcular o raio |
| Flexibilidade | "different ways to form" (linha 1523) | Valor recomendado |

**PossÃ­veis definiÃ§Ãµes (nÃ£o especificadas no paper):**

| Abordagem | FÃ³rmula | CaracterÃ­stica |
|-----------|---------|----------------|
| Raio fixo | r = constante | Simples, nÃ£o adapta |
| Baseado em Ïƒ | r = c Ã— Ïƒ | Adapta Ã  dispersÃ£o |
| Baseado em Ï„ | Ï„_local > threshold | Consistente com TEDA |
| k-vizinhos | dist. ao k-Ã©simo vizinho | Adapta Ã  densidade local |

**ImplicaÃ§Ã£o:** O paper Angelov (2014) Ã© um **framework conceitual**. A implementaÃ§Ã£o prÃ¡tica de clustering requer decisÃµes adicionais que o paper deixa em aberto.

**ProvÃ¡vel soluÃ§Ã£o:** MicroTEDAclus (Maia 2020) provavelmente preenche essa lacuna com critÃ©rios especÃ­ficos de formaÃ§Ã£o/merge/split de clusters.

---

## 9. RelaÃ§Ã£o com Minha Pesquisa

### 9.1 Base para MicroTEDAclus (Maia et al., 2020)

MicroTEDAclus **estende** TEDA com:
- Micro-clusters em vez de pontos individuais
- Mixture of typicalities
- OperaÃ§Ãµes de merge/split para clusters

As fÃ³rmulas de eccentricidade e tipicalidade do MicroTEDAclus derivam diretamente deste paper.

### 9.2 AplicaÃ§Ã£o em IDS IoT

| Vantagem TEDA | AplicaÃ§Ã£o em IDS |
|---------------|------------------|
| Sem prior assumptions | NÃ£o precisa assumir distribuiÃ§Ã£o do trÃ¡fego |
| Recursivo | Adequado para streaming em tempo real |
| Detecta anomalias | Identifica ataques como pontos excÃªntricos |
| Usa dependÃªncia | Captura correlaÃ§Ã£o temporal do trÃ¡fego |

**DiferenÃ§a de regras de especialista:**
- Regras (Snort/Suricata): Detectam ataques **conhecidos**
- TEDA: Pode detectar ataques **novos** (zero-day) como pontos atÃ­picos

### 9.3 ContribuiÃ§Ã£o para DissertaÃ§Ã£o

Este paper Ã© **fundamental** para:
- **FundamentaÃ§Ã£o TeÃ³rica:** Define os conceitos matemÃ¡ticos de Î¾ e Ï„
- **Metodologia:** Justifica abordagem nÃ£o-probabilÃ­stica para dados de rede
- **ImplementaÃ§Ã£o:** FÃ³rmulas recursivas para sistema em tempo real

---

## 10. CitaÃ§Ãµes Importantes

> "Unlike purely random processes, such as throwing dices, tossing coins... real life processes of interest **do violate** the main assumptions which the traditional probability theory requires." (linhas 32-36)

> "It does not require independence of the individual data samples; on the contrary, the proposed approach **builds upon their mutual dependence**." (linhas 90-93)

> "The proposed new framework TEDA is a systematic methodology which **does not require prior assumptions** and can be used for development of a range of methods for anomalies and fault detection, image processing, clustering, classification, prediction, control, filtering, regression, etc." (linhas 43-48)

> "For such pure random data the **traditional probability theory is the best tool** to be used. However, for real data processes â€“ which are the majority of the cases â€“ we argue that TEDA is better justified." (linhas 101-106)

---

## 11. ReferÃªncias Relevantes do Paper

| # | ReferÃªncia | Por que Ã© relevante |
|---|------------|---------------------|
| [2] | Osherson & Smith (1997) "On typicality and vagueness" | Conceito filosÃ³fico de tipicalidade |
| [3] | Angelov (2012) "Autonomous Learning Systems" | Livro com detalhes de TEDA e sistemas evolutivos |
| [7] | Zadeh (1965) "Fuzzy sets" | ComparaÃ§Ã£o com funÃ§Ãµes de pertinÃªncia fuzzy |
| [8] | Angelov & Yager (2012) "AnYa framework" | Introduz conceito de "data clouds" |

---

## 12. Notas Pessoais

### Insights da Leitura

1. **Honestidade cientÃ­fica:** Angelov reconhece que TEDA nÃ£o Ã© para tudo â€” probabilidade Ã© melhor para processos puramente aleatÃ³rios

2. **Filosofia central:** Probabilidade pergunta "quantas vezes X apareceu?"; TEDA pergunta "onde X estÃ¡ em relaÃ§Ã£o aos outros?"

3. **Para IDS:** TrÃ¡fego de rede NÃƒO Ã© como jogar dados â€” tem correlaÃ§Ã£o temporal, padrÃµes de uso. TEDA Ã© adequado.

4. **Simplicidade:** As fÃ³rmulas sÃ£o elegantes e computacionalmente eficientes (recursivas)

### DÃºvidas Esclarecidas

- **Frequentista:** Probabilidade como frequÃªncia relativa no limite infinito
- **Kernels:** FunÃ§Ãµes para estimar densidade (Gaussiano, Epanechnikov) â€” TEDA nÃ£o precisa
- **Mutual dependence:** TEDA usa distÃ¢ncias entre todos os pares, nÃ£o trata pontos como independentes

---

## 13. Checklist de Leitura

- [x] Li o abstract
- [x] Li a introduÃ§Ã£o
- [x] Entendi a definiÃ§Ã£o de eccentricity
- [x] Entendi a definiÃ§Ã£o de typicality
- [x] Copiei as fÃ³rmulas principais
- [x] Entendi o algoritmo TEDA
- [x] Li os experimentos
- [x] Li a conclusÃ£o
- [ ] Identifiquei relaÃ§Ã£o completa com MicroTEDAclus
- [x] Identifiquei aplicaÃ§Ã£o na minha pesquisa

---

## 14. GlossÃ¡rio de Termos

| Termo | DefiniÃ§Ã£o |
|-------|-----------|
| **Frequentista** | Abordagem que define probabilidade como frequÃªncia relativa em infinitas repetiÃ§Ãµes |
| **Prior assumption** | SuposiÃ§Ã£o feita antes de ver os dados (ex: assumir distribuiÃ§Ã£o Gaussiana) |
| **Kernel** | FunÃ§Ã£o usada para estimar densidade de dados (ex: Gaussiano, Epanechnikov) |
| **Bandwidth** | ParÃ¢metro que controla a largura do kernel |
| **Eccentricity (Î¾)** | Medida de quÃ£o "excÃªntrico" (distante dos outros) um ponto Ã© |
| **Typicality (Ï„)** | Medida de quÃ£o "tÃ­pico" (prÃ³ximo ao padrÃ£o) um ponto Ã© |
| **Mutual dependence** | RelaÃ§Ã£o espacial entre pontos de dados â€” quem estÃ¡ perto de quem |
| **Data cloud** | Agrupamento de dados sem forma, parÃ¢metros ou fronteiras especÃ­ficas |
| **First principles** | Modelos derivados de leis fundamentais (fÃ­sica, quÃ­mica, etc.) |
| **Euclidean distance** | DistÃ¢ncia em linha reta: âˆšÎ£(a-b)Â² â€” sensÃ­vel a escala |
| **Manhattan distance** | Soma dos deslocamentos por eixo: Î£\|a-b\| â€” robusta a outliers |
| **Mahalanobis distance** | DistÃ¢ncia que considera correlaÃ§Ã£o via matriz de covariÃ¢ncia |
| **Cosine distance** | Mede Ã¢ngulo entre vetores, ignora magnitude: 1 - cos(Î¸) |
| **Belief functions** | Teoria de Dempster-Shafer â€” graus de crenÃ§a em conjuntos de eventos |
| **Possibility theory** | Teoria que distingue possibilidade (Î ) de necessidade (N) |
| **NormalizaÃ§Ã£o** | Transformar valores para escala comum, permitindo comparaÃ§Ãµes justas |
| **NormalizaÃ§Ã£o por soma** | x/Î£x â€” valores como fraÃ§Ã£o do total (soma = 1) |
| **NormalizaÃ§Ã£o Min-Max** | (x-min)/(max-min) â€” mapeia para intervalo [0,1] |
| **Z-Score** | (x-Î¼)/Ïƒ â€” mede em unidades de desvio padrÃ£o |
| **Teorema de Huygens-Steiner** | Identidade que relaciona distÃ¢ncias pairwise com distÃ¢ncia Ã  mÃ©dia: Î£d_ijÂ² = kÂ·d_Î¼Â² + kÂ·ÏƒÂ². Base da fÃ³rmula recursiva do TEDA |
| **FÃ³rmula de KÃ¶nig-Huygens** | RelaÃ§Ã£o estatÃ­stica Var(X) = E[XÂ²] - E[X]Â², usada para expressar ÏƒÂ² = X - \|\|Î¼\|\|Â² |
| **Produto escalar X** | MÃ©dia dos quadrados das normas: X = (1/k)Â·Î£\|\|x_i\|\|Â². Usado para calcular variÃ¢ncia incrementalmente |

---

**Status:** âœ… Completo (~95%)
**Ãšltima atualizaÃ§Ã£o:** 2026-01-05
**PrÃ³ximos passos:** Leitura do MicroTEDAclus (Maia 2020) â€” fichamento separado
