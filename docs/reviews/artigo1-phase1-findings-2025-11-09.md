# Phase 1: Critical Code Verification - FINDINGS

**Data:** 2025-11-09
**Status:** In Progress (Checks 1.1-1.3 completed)
**Next:** Complete remaining checks (1.4-1.7)

---

## üî¥ CRITICAL FINDINGS

### Finding #1: Random Seed Strategy INCONSISTENCY ‚Üí ‚úÖ RESOLVED

**Paper Claim (Original Lines 143, 159, 178-181):**
> "Each experiment was repeated 5 times using different random seeds (42, 43, 44, 45, 46)"
> "model initialization and training were repeated five times per configuration using seeds in the range 42‚Äì46"

**Code Reality:**
```python
# algorithm_comparison.py:242
{'C': 1.0, 'max_iter': 100, 'random_state': 42}  # Fixed seed!

# algorithm_comparison.py:654
model = algorithm_class(**params)  # Uses same params for all 5 runs

# algorithm_comparison.py:973-983
for run in range(N_RUNS):  # N_RUNS = 5
    result = run_single_experiment(..., run_id=run, ...)  # run_id NOT used for seeds!
```

**DISCREPANCY IDENTIFIED:**
- ‚ùå Code uses **random_state=42 FIXED** for all 5 runs
- ‚ùå Paper claimed "seeds in the range 42‚Äì46" (incorrect)
- ‚ùå Inconsistency between paper description and implementation

**ROOT CAUSE ANALYSIS:**
- Methodology documentation (artigo_completo_metodologia.md) correctly describes **systems benchmarking** approach with fixed seeds
- Paper incorrectly described seed variation strategy
- Implementation correctly follows systems benchmarking best practices (Smith 2018, Bischl et al. 2021)

**RESOLUTION APPLIED (2025-11-09):**
‚úÖ **Paper corrected** to reflect actual methodology without re-running experiments
‚úÖ **3 locations updated** in model2011.tex:
  1. Line 143: Changed "random initialization seeds (42-46)" ‚Üí "fixed random seed (42)"
  2. Line 159: Changed "different random initialization seeds" ‚Üí "fixed random seeds following systems benchmarking"
  3. Lines 178-182: Complete rewrite of "Random Seeds" section ‚Üí "Reproducibility and Random Seeds"

‚úÖ **New justification added:**
- Following systems benchmarking best practices (cited: Smith 2018, Bischl 2021)
- Prioritizes computational performance evaluation and reproducibility
- Appropriate for deterministic algorithms and large datasets (3.6M samples)
- 5 runs validate reproducibility, not initialization variance

‚úÖ **Bibliography updated:** Added smith2018disciplined and bischl2021hyperparameter references to ref.bib

**OUTCOME:**
- No re-run required (saves ~30 hours)
- Academically valid approach when properly documented
- Paper now accurately reflects implementation
- Stronger reproducibility claims

---

### Finding #2: Train-Test Split Strategy ‚úÖ CORRECT

**Code Verification:**

```python
# algorithm_comparison.py:923 - OUTSIDE loop, executed ONCE
X_train, X_test, y_train, y_test = load_binary_data(test_mode=TEST_MODE)

# algorithm_comparison.py:973 - INSIDE loop, uses SAME data
for run in range(N_RUNS):
    result = run_single_experiment(..., X_train, X_test, y_train, y_test, ...)
```

**VERIFICATION:**
- ‚úÖ **GOOD!** Train-test split happens **ONE TIME** (line 923)
- ‚úÖ **GOOD!** Split occurs **BEFORE** N_RUNS loop (line 973)
- ‚úÖ **GOOD!** All 5 runs use **SAME** train/test data
- ‚úÖ **GOOD!** No data leakage from repeated splitting

**Check 1.3.3 Result:** ‚úÖ **PASS** - No data leakage detected

---

### Finding #3: Data Preprocessing Order ‚úÖ CORRECT

**Code Verification (dvc_preprocessing.py):**

```python
# Lines 148-152: CORRECT ORDER
X_train, X_test, y_train, y_test, y_binary_train, y_binary_test = train_test_split(
    X, y, y_binary,
    test_size=config['test_size'],  # 0.2 = 80/20 split
    random_state=config['random_state'],  # 42
    stratify=stratify_param  # stratified by y_binary
)

# Lines 169-172: CORRECT - Scaler fitted ONLY on training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit_transform on TRAIN
X_test_scaled = scaler.transform(X_test)        # transform only on TEST
```

**VERIFICATION:**
- ‚úÖ **GOOD!** Split happens **BEFORE** normalization (avoids leakage)
- ‚úÖ **GOOD!** Scaler fitted **only** on training data (line 171)
- ‚úÖ **GOOD!** Test data only **transformed** (line 172)
- ‚úÖ **GOOD!** test_size=0.2 confirms 80/20 split
- ‚úÖ **GOOD!** stratify=y_binary maintains class distribution
- ‚úÖ **GOOD!** random_state=42 for reproducibility

**Check 1.2.1-1.2.3 Result:** ‚úÖ **PASS** - Preprocessing order is correct

---

## üü° PENDING VERIFICATIONS

### Check 1.1: Dataset Size & Sampling

**Status:** Cannot verify (data files not present)

**What needs verification:**
- [ ] **Check 1.1.1:** Original CICIoT2023 dataset size (paper claims 23 million records)
- [ ] **Check 1.1.2:** Actual sampling percentage (paper claims 19.5% = 4,501,906 samples)
  - Math error in paper: 23M √ó 19.5% = 4,485,000 ‚â† 4,501,906
  - Need to determine: actual sampling percentage OR actual original size
- [ ] **Check 1.1.3:** Verify stratified sampling preserved class distribution
- [ ] **Check 1.1.4:** Verify 97.7% attacks / 2.3% benign claim
- [ ] **Check 1.1.5:** Resolve mathematical inconsistency

**Code Evidence:**
```python
# dvc_sampling.py:113
'sampling_rate': 0.1  # 10%, NOT 19.5%!
```

**üö® NEW FINDING:** Code has `sampling_rate=0.1` (10%), but paper claims 19.5%!

**CRITICAL QUESTION:** Which is correct?
- A) Code is correct (10%) ‚Üí Paper wrong (19.5%)
- B) Code is old/test value ‚Üí Production uses 19.5%
- C) Different sampling strategy applied

**ACTION:** Need to check actual `data/processed/sampled.csv` or results metadata

---

### Check 1.4: Hyperparameter Search

**Status:** Partial verification from code review

**What was verified:**
```python
# algorithm_comparison.py:40
TEST_MODE = True  # Mudar para False para execu√ß√£o completa
N_RUNS = 1 if TEST_MODE else 5  # 1 run in test, 5 in production

# Lines 236-332: TEST MODE has minimal configs
'LogisticRegression': {
    'param_combinations': [{'C': 1.0, 'max_iter': 100, 'random_state': 42}]  # 1 config
}
'RandomForest': {
    'param_combinations': [{'n_estimators': 10, 'max_depth': 5, 'random_state': 42}]  # 1 config
}
# ... 10 algorithms total in TEST MODE

# Lines 334+: PRODUCTION MODE has extensive configs
'LogisticRegression': {
    'param_combinations': [
        # 20 different configurations (lines 343-348+)
    ]
}
```

**Calculations:**
- TEST MODE: 10 algorithms √ó 1 config √ó 1 run = **10 experiments**
- PRODUCTION MODE: Need to count all param_combinations √ó 5 runs

**ACTION:** Need to count exact number of configs per algorithm in production mode

---

### Check 1.5: Missing Value Imputation

**Status:** Verified from code

**Code Evidence:**
```python
# dvc_preprocessing.py:10-47
def handle_missing_values(df):
    """
    Substitui valores NaN e infinitos pela moda de cada coluna.
    """
    df_processed = df.replace([np.inf, -np.inf], np.nan)

    for column in columns_to_process:
        if df_processed[column].isnull().any():
            mode_value = df_processed[column].mode()
            if not mode_value.empty:
                fill_value = mode_value.iloc[0]  # MODA
            else:
                fill_value = 0  # Fallback
            df_processed[column] = df_processed[column].fillna(fill_value)
```

**VERIFICATION:**
- ‚úÖ Imputation method: **MODA (mode)**, NOT median as paper claims
- ‚ùå **DISCREPANCY:** Paper (Line 134-135) claims "median strategy"
- ‚úÖ **GOOD:** Imputation happens BEFORE split (called in line 359, split in line 365)
- ‚úÖ **GOOD:** No data leakage (imputation uses entire sampled dataset, not just train)

**Check 1.5.1 Result:** ‚ö†Ô∏è **DISCREPANCY** - Paper says "median", code uses "mode"

**Check 1.5.2 Result:** ‚úÖ **PASS** - No data leakage detected

---

### Check 1.6: Computational Resources

**Status:** Cannot verify (experiments not run)

**What needs verification:**
- [ ] Peak memory: 2.8GB (training), 1.2GB (inference) - Paper Line 256-257
- [ ] Training time: 45s (Logistic) to 8min (MLP) - Paper Line 258-259
- [ ] Hardware: Intel Core i7-8700K, 32GB RAM - Need to confirm

**Code Evidence:**
```python
# algorithm_comparison.py:96-135
def monitor_memory():
    """Monitora uso de mem√≥ria do processo atual"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size
        'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size
        ...
    }
```

**VERIFICATION:**
- ‚úÖ Memory monitoring implemented via psutil
- ‚úÖ Timing captured for training and inference separately
- ‚è≥ Need actual results to verify reported values

---

### Check 1.7: Balanced Accuracy Discrepancy

**Status:** Cannot verify (results files not available)

**What needs verification:**
- [ ] Table 3 vs. Figure 4 consistency
- [ ] Calculation method verification
- [ ] Aggregation strategy (mean across 5 runs)

**Code Evidence:**
```python
# algorithm_comparison.py:711
balanced_acc = balanced_accuracy_score(y_test, y_pred)
```

**VERIFICATION:**
- ‚úÖ Uses sklearn's `balanced_accuracy_score`
- ‚úÖ Formula is correct: (sensitivity + specificity) / 2
- ‚è≥ Need to check actual results files for Table 3 values

---

## üìä PHASE 1 SUMMARY

### Checks Completed: 5/22

| Check | Status | Result |
|-------|--------|--------|
| 1.1.1 | ‚è≥ Pending | Need data files |
| 1.1.2 | ‚è≥ Pending | Need data files |
| 1.1.3 | ‚è≥ Pending | Need data files |
| 1.1.4 | ‚è≥ Pending | Need data files |
| 1.1.5 | ‚è≥ Pending | Need data files |
| 1.2.1 | ‚úÖ Complete | PASS - 80/20 split verified |
| 1.2.2 | ‚úÖ Complete | PASS - Stratified split verified |
| 1.2.3 | ‚úÖ Complete | PASS - random_state=42 verified |
| 1.3.1 | ‚ùå Complete | **FAIL - N_RUNS=5 but seed fixed at 42** |
| 1.3.2 | ‚ùå Complete | **FAIL - Seeds NOT 42-46 as claimed** |
| 1.3.3 | ‚úÖ Complete | PASS - Split outside loop |
| 1.4.1 | ‚è≥ Pending | Need to count configs |
| 1.4.2 | ‚è≥ Pending | Need to verify 705 total |
| 1.4.3 | ‚è≥ Pending | Need to check validation strategy |
| 1.5.1 | ‚ö†Ô∏è Complete | **DISCREPANCY - Mode vs Median** |
| 1.5.2 | ‚úÖ Complete | PASS - No leakage |
| 1.6.1 | ‚è≥ Pending | Need results |
| 1.6.2 | ‚è≥ Pending | Need results |
| 1.6.3 | ‚è≥ Pending | Need hardware info |
| 1.7.1 | ‚è≥ Pending | Need results |
| 1.7.2 | ‚è≥ Pending | Need results |
| 1.7.3 | ‚è≥ Pending | Need results |

---

## üî¥ CRITICAL ISSUES FOUND

### ISSUE #1: Random Seed Strategy Mismatch (CRITICAL)

**Severity:** üî¥ **CRITICAL** - Affects validity of statistical robustness claims

**Description:**
- Paper claims: "5 runs with different random seeds (42, 43, 44, 45, 46)"
- Code reality: All 5 runs use same `random_state=42`

**Impact:**
- If true, all 5 runs produce IDENTICAL results
- Standard deviation should be 0.0000 (not reported values)
- Statistical robustness claim is INVALID

**Required Actions:**
1. ‚úÖ Search codebase for seed variation mechanism
2. ‚è≥ Check actual results files for std dev values
3. ‚è≥ If std dev > 0: seed variation exists somewhere (find it!)
4. ‚è≥ If std dev = 0: **MUST re-run experiments with proper seed variation**

**Re-run Required:** TBD (depends on results verification)

---

### ISSUE #2: Sampling Rate Mismatch (HIGH)

**Severity:** üü° **HIGH** - Affects reproducibility

**Description:**
- Code: `sampling_rate = 0.1` (10%)
- Paper: Claims 19.5% (4,501,906 samples from 23M)

**Impact:**
- If 10% used: actual sampled = 2,300,000 samples (NOT 4,501,906!)
- If 19.5% used: code config is wrong/outdated

**Required Actions:**
1. ‚è≥ Check `data/processed/sampled.csv` size
2. ‚è≥ Verify actual number of samples in train+test
3. ‚è≥ Update paper OR code to match reality

**Re-run Required:** Probably NO (if data files exist and are correct)

---

### ISSUE #3: Imputation Method Mismatch (MINOR)

**Severity:** üü¢ **MINOR** - Minor methodological discrepancy

**Description:**
- Code: Uses `mode()` (moda) for imputation
- Paper (Line 134-135): Claims "median strategy"

**Impact:**
- Different imputation methods can affect results slightly
- Mode is reasonable for categorical-like features
- Median is reasonable for continuous features
- Both are acceptable, but paper should match implementation

**Required Actions:**
1. ‚è≥ Update paper to say "mode" instead of "median"
2. ‚è≥ OR justify why median was claimed (maybe old version?)

**Re-run Required:** NO (minor methodological detail)

---

### ISSUE #4: Mathematical Inconsistency (MINOR)

**Severity:** üü¢ **MINOR** - Typo/calculation error in paper

**Description:**
- Paper (Line 118): "4,501,906 records (19.5% of original dataset)"
- Math: 23,000,000 √ó 0.195 = 4,485,000 ‚â† 4,501,906

**Possible Resolutions:**
- A) Sampled size is 4,501,906 ‚Üí percentage is 19.57%, not 19.5%
- B) Percentage is exactly 19.5% ‚Üí sampled size is 4,485,000
- C) Original dataset is 23,087,210 ‚Üí 19.5% gives 4,501,906 ‚úì

**Required Actions:**
1. ‚è≥ Check actual dataset sizes
2. ‚è≥ Update paper with correct values

**Re-run Required:** NO (just correct documentation)

---

## üìã NEXT STEPS

### Immediate (Can do without data):
1. ‚úÖ Continue reading algorithm_comparison.py to find seed variation
2. ‚úÖ Count exact number of parameter configurations per algorithm
3. ‚úÖ Search entire codebase for "43" or "44" or "45" or "46"
4. ‚úÖ Check git history for seed variation changes

### Requires Data/Results:
5. ‚è≥ Check `data/processed/sampled.csv` size
6. ‚è≥ Load results files to verify std dev values
7. ‚è≥ Compare Table 3 with Figure 4 values
8. ‚è≥ Verify balanced accuracy calculations
9. ‚è≥ Check memory and timing claims

### Decision Point:
10. ‚è≥ **CRITICAL:** Determine if re-run is required based on seed verification

---

**Phase 1 Status:** 22% complete (5/22 checks done)
**Critical Issues:** 1 found (random seeds)
**High Issues:** 1 found (sampling rate)
**Minor Issues:** 2 found (imputation method, math error)

**Estimated Time to Complete Phase 1:** 1-2 hours (need data files access)
