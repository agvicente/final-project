# Framework Pr√°tico para EDA - Dataset CICIoT2023

**Objetivo:** Implementar uma an√°lise explorat√≥ria de dados sistem√°tica e academicamente fundamentada para o dataset CICIoT2023.

---

## üìã Checklist de Implementa√ß√£o

### ‚úÖ Pr√©-requisitos
- [ ] Python 3.8+
- [ ] Bibliotecas: pandas, numpy, matplotlib, seaborn, scipy, scikit-learn
- [ ] Dados CICIoT2023 na pasta `datasets/CSV/MERGED_CSV/`
- [ ] Notebook Jupyter configurado

### ‚úÖ Estrutura do Projeto
```
src/eda/
‚îú‚îÄ‚îÄ EDA_Lab_CICIoT2023.md          # Documenta√ß√£o te√≥rica
‚îú‚îÄ‚îÄ EDA_Framework_Pratico.md       # Este arquivo
‚îú‚îÄ‚îÄ eda_implementation.ipynb       # Implementa√ß√£o principal
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py            # Carregamento de dados
‚îÇ   ‚îú‚îÄ‚îÄ statistical_analysis.py   # An√°lises estat√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py          # Visualiza√ß√µes
‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py       # Gera√ß√£o de relat√≥rios
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ figures/                  # Gr√°ficos gerados
    ‚îú‚îÄ‚îÄ tables/                   # Tabelas estat√≠sticas
    ‚îî‚îÄ‚îÄ reports/                  # Relat√≥rios finais
```

---

## üöÄ Implementa√ß√£o Passo a Passo

### FASE 1: Prepara√ß√£o do Ambiente

#### Passo 1.1: Instala√ß√£o de Depend√™ncias
```bash
pip install pandas numpy matplotlib seaborn scipy scikit-learn
pip install plotly jupyter-dash # Para visualiza√ß√µes interativas
pip install memory-profiler # Para monitoramento de mem√≥ria
```

#### Passo 1.2: Configura√ß√£o do Notebook
```python
# Configura√ß√µes iniciais
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path
import sys
import os

# Configura√ß√µes de visualiza√ß√£o
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12
sns.set_style("whitegrid")
warnings.filterwarnings('ignore')

# Configurar seed para reprodutibilidade
np.random.seed(42)

# Configurar pandas para exibir mais colunas
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)
```

### FASE 2: Carregamento e Consolida√ß√£o dos Dados

#### Passo 2.1: Fun√ß√£o de Carregamento Otimizada
```python
def load_ciciot_datasets(data_path='datasets/CSV/MERGED_CSV/', 
                        sample_size=None, 
                        files_to_load=None):
    """
    Carrega e consolida os datasets CICIoT2023
    
    Args:
        data_path: Caminho para os arquivos CSV
        sample_size: Tamanho da amostra (None para carregar tudo)
        files_to_load: Lista de arquivos espec√≠ficos (None para todos)
    
    Returns:
        DataFrame consolidado
    """
    data_path = Path(data_path)
    datasets = []
    
    # Listar arquivos dispon√≠veis
    if files_to_load is None:
        csv_files = sorted(data_path.glob('Merged*.csv'))
    else:
        csv_files = [data_path / f for f in files_to_load]
    
    print(f"Carregando {len(csv_files)} arquivos...")
    
    total_rows = 0
    for i, file_path in enumerate(csv_files):
        try:
            print(f"  {i+1}/{len(csv_files)}: {file_path.name}")
            
            # Carregar com chunking para grandes arquivos
            chunk_size = 10000
            chunks = []
            
            for chunk in pd.read_csv(file_path, chunksize=chunk_size):
                if sample_size and total_rows >= sample_size:
                    break
                chunks.append(chunk)
                total_rows += len(chunk)
            
            if chunks:
                df = pd.concat(chunks, ignore_index=True)
                datasets.append(df)
                
        except Exception as e:
            print(f"    Erro ao carregar {file_path}: {e}")
    
    # Consolidar todos os datasets
    if datasets:
        full_dataset = pd.concat(datasets, ignore_index=True)
        
        # Aplicar amostragem se especificada
        if sample_size and len(full_dataset) > sample_size:
            full_dataset = full_dataset.sample(n=sample_size, random_state=42)
        
        print(f"Dataset consolidado: {full_dataset.shape}")
        return full_dataset
    else:
        raise ValueError("Nenhum dataset foi carregado com sucesso")

# Exemplo de uso
df = load_ciciot_datasets(sample_size=100000)  # Amostra de 100k para teste
```

#### Passo 2.2: Verifica√ß√£o Inicial dos Dados
```python
def initial_data_inspection(df):
    """Inspe√ß√£o inicial dos dados"""
    print("=" * 50)
    print("INSPE√á√ÉO INICIAL DOS DADOS")
    print("=" * 50)
    
    # Informa√ß√µes b√°sicas
    print(f"Dimens√µes: {df.shape}")
    print(f"Uso de mem√≥ria: {df.memory_usage().sum() / 1024**2:.2f} MB")
    print(f"Per√≠odo dos dados: {pd.to_datetime(df['ts'], unit='s').min()} a {pd.to_datetime(df['ts'], unit='s').max()}")
    
    # Tipos de dados
    print("\nTipos de dados:")
    print(df.dtypes.value_counts())
    
    # Valores ausentes
    missing_values = df.isnull().sum()
    if missing_values.sum() > 0:
        print(f"\nValores ausentes: {missing_values.sum()}")
        print(missing_values[missing_values > 0])
    else:
        print("\nNenhum valor ausente encontrado")
    
    # Valores duplicados
    duplicates = df.duplicated().sum()
    print(f"Registros duplicados: {duplicates} ({duplicates/len(df)*100:.2f}%)")
    
    # Informa√ß√µes sobre as colunas
    print(f"\nColunas ({len(df.columns)}):")
    for col in df.columns:
        print(f"  - {col}")
    
    return df.info()

# Executar inspe√ß√£o inicial
initial_data_inspection(df)
```

### FASE 3: An√°lise Descritiva Univariada

#### Passo 3.1: An√°lise de Distribui√ß√£o de Classes
```python
def analyze_class_distribution(df, label_col='label'):
    """An√°lise detalhada da distribui√ß√£o de classes"""
    print("=" * 50)
    print("AN√ÅLISE DE DISTRIBUI√á√ÉO DE CLASSES")
    print("=" * 50)
    
    # Contagem de classes
    class_counts = df[label_col].value_counts()
    class_percentages = df[label_col].value_counts(normalize=True) * 100
    
    # Criar DataFrame para visualiza√ß√£o
    class_summary = pd.DataFrame({
        'Count': class_counts,
        'Percentage': class_percentages
    }).sort_values('Count', ascending=False)
    
    print("Distribui√ß√£o de classes:")
    print(class_summary)
    
    # Visualiza√ß√µes
    fig, axes = plt.subplots(2, 2, figsize=(20, 15))
    
    # Gr√°fico de barras
    class_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')
    axes[0,0].set_title('Distribui√ß√£o de Classes - Contagem')
    axes[0,0].set_ylabel('Frequ√™ncia')
    axes[0,0].tick_params(axis='x', rotation=45)
    
    # Gr√°fico de pizza
    class_counts.plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%')
    axes[0,1].set_title('Distribui√ß√£o de Classes - Propor√ß√£o')
    axes[0,1].set_ylabel('')
    
    # Log scale para melhor visualiza√ß√£o
    class_counts.plot(kind='bar', ax=axes[1,0], color='lightcoral', logy=True)
    axes[1,0].set_title('Distribui√ß√£o de Classes - Escala Log')
    axes[1,0].set_ylabel('Frequ√™ncia (log)')
    axes[1,0].tick_params(axis='x', rotation=45)
    
    # Gr√°fico de barras horizontal
    class_counts.plot(kind='barh', ax=axes[1,1], color='lightgreen')
    axes[1,1].set_title('Distribui√ß√£o de Classes - Horizontal')
    axes[1,1].set_xlabel('Frequ√™ncia')
    
    plt.tight_layout()
    plt.show()
    
    # An√°lise de balanceamento
    total_samples = len(df)
    majority_class = class_counts.index[0]
    minority_class = class_counts.index[-1]
    
    imbalance_ratio = class_counts.max() / class_counts.min()
    
    print(f"\nAn√°lise de Balanceamento:")
    print(f"Classe majorit√°ria: {majority_class} ({class_counts.max():,} samples)")
    print(f"Classe minorit√°ria: {minority_class} ({class_counts.min():,} samples)")
    print(f"Raz√£o de desbalanceamento: {imbalance_ratio:.2f}:1")
    
    return class_summary

# Executar an√°lise de classes
class_analysis = analyze_class_distribution(df)
```

#### Passo 3.2: An√°lise Estat√≠stica Descritiva
```python
def comprehensive_statistical_analysis(df):
    """An√°lise estat√≠stica completa das features num√©ricas"""
    print("=" * 50)
    print("AN√ÅLISE ESTAT√çSTICA DESCRITIVA")
    print("=" * 50)
    
    # Separar features num√©ricas
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # Remover timestamp se presente
    if 'ts' in numeric_features:
        numeric_features.remove('ts')
    
    print(f"Analisando {len(numeric_features)} features num√©ricas...")
    
    # Estat√≠sticas descritivas b√°sicas
    desc_stats = df[numeric_features].describe()
    
    # Estat√≠sticas adicionais
    additional_stats = pd.DataFrame({
        'Skewness': df[numeric_features].skew(),
        'Kurtosis': df[numeric_features].kurtosis(),
        'CV': df[numeric_features].std() / df[numeric_features].mean(),
        'IQR': df[numeric_features].quantile(0.75) - df[numeric_features].quantile(0.25)
    })
    
    # Combinar estat√≠sticas
    full_stats = pd.concat([desc_stats.T, additional_stats], axis=1)
    
    # Salvar estat√≠sticas
    full_stats.to_csv('src/eda/results/tables/descriptive_statistics.csv')
    
    # Identificar features com comportamento especial
    print("\nFeatures com alta variabilidade (CV > 1):")
    high_cv = additional_stats[additional_stats['CV'] > 1].sort_values('CV', ascending=False)
    print(high_cv)
    
    print("\nFeatures com assimetria alta (|Skewness| > 2):")
    high_skew = additional_stats[abs(additional_stats['Skewness']) > 2].sort_values('Skewness', ascending=False)
    print(high_skew)
    
    # Visualiza√ß√£o das distribui√ß√µes
    plot_feature_distributions(df, numeric_features[:16])  # Primeiras 16 features
    
    return full_stats

def plot_feature_distributions(df, features):
    """Plota distribui√ß√µes das features"""
    n_features = len(features)
    n_cols = 4
    n_rows = (n_features + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))
    axes = axes.flatten()
    
    for i, feature in enumerate(features):
        df[feature].hist(bins=50, ax=axes[i], alpha=0.7, color='skyblue')
        axes[i].set_title(f'{feature}')
        axes[i].set_xlabel('Valor')
        axes[i].set_ylabel('Frequ√™ncia')
        
        # Adicionar linha da m√©dia
        mean_val = df[feature].mean()
        axes[i].axvline(mean_val, color='red', linestyle='--', label=f'M√©dia: {mean_val:.2f}')
        axes[i].legend()
    
    # Remover subplots extras
    for i in range(n_features, len(axes)):
        axes[i].remove()
    
    plt.tight_layout()
    plt.show()

# Executar an√°lise estat√≠stica
statistical_analysis = comprehensive_statistical_analysis(df)
```

### FASE 4: An√°lise Bivariada

#### Passo 4.1: An√°lise de Correla√ß√µes
```python
def correlation_analysis(df):
    """An√°lise completa de correla√ß√µes"""
    print("=" * 50)
    print("AN√ÅLISE DE CORRELA√á√ïES")
    print("=" * 50)
    
    # Selecionar features num√©ricas
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'ts' in numeric_features:
        numeric_features.remove('ts')
    
    # Calcular matriz de correla√ß√£o
    correlation_matrix = df[numeric_features].corr()
    
    # Visualiza√ß√£o da matriz de correla√ß√£o
    plt.figure(figsize=(20, 16))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', 
                center=0, square=True, linewidths=0.5)
    plt.title('Matriz de Correla√ß√£o - Features CICIoT2023')
    plt.tight_layout()
    plt.show()
    
    # Identificar correla√ß√µes altas
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > 0.7:  # Threshold para alta correla√ß√£o
                high_corr_pairs.append({
                    'Feature1': correlation_matrix.columns[i],
                    'Feature2': correlation_matrix.columns[j],
                    'Correlation': corr_val
                })
    
    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', 
                                                           key=abs, ascending=False)
    
    print(f"\nPares de features com alta correla√ß√£o (|r| > 0.7):")
    print(high_corr_df)
    
    # Salvar resultados
    correlation_matrix.to_csv('src/eda/results/tables/correlation_matrix.csv')
    high_corr_df.to_csv('src/eda/results/tables/high_correlations.csv', index=False)
    
    return correlation_matrix, high_corr_df

# Executar an√°lise de correla√ß√µes
corr_matrix, high_corr = correlation_analysis(df)
```

#### Passo 4.2: An√°lise por Classe de Ataque
```python
def attack_class_analysis(df, label_col='label'):
    """An√°lise das features por classe de ataque"""
    print("=" * 50)
    print("AN√ÅLISE POR CLASSE DE ATAQUE")
    print("=" * 50)
    
    # Selecionar features num√©ricas importantes
    important_features = ['flow_duration', 'Tot_size', 'Rate', 'Srate', 'Drate', 
                         'AVG', 'Min', 'Max', 'Std']
    
    # Estat√≠sticas por classe
    class_stats = df.groupby(label_col)[important_features].agg(['mean', 'std', 'median']).round(4)
    
    # Visualiza√ß√µes comparativas
    n_features = len(important_features)
    n_cols = 3
    n_rows = (n_features + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6 * n_rows))
    axes = axes.flatten()
    
    for i, feature in enumerate(important_features):
        df.boxplot(column=feature, by=label_col, ax=axes[i])
        axes[i].set_title(f'{feature} por Classe de Ataque')
        axes[i].set_xlabel('Classe de Ataque')
        axes[i].set_ylabel(feature)
        axes[i].tick_params(axis='x', rotation=45)
    
    # Remover subplots extras
    for i in range(n_features, len(axes)):
        axes[i].remove()
    
    plt.tight_layout()
    plt.show()
    
    # Teste ANOVA para verificar diferen√ßas significativas
    from scipy.stats import f_oneway
    
    anova_results = []
    classes = df[label_col].unique()
    
    for feature in important_features:
        groups = [df[df[label_col] == cls][feature].dropna() for cls in classes]
        f_stat, p_value = f_oneway(*groups)
        anova_results.append({
            'Feature': feature,
            'F_statistic': f_stat,
            'p_value': p_value,
            'Significant': p_value < 0.05
        })
    
    anova_df = pd.DataFrame(anova_results)
    
    print("\nResultados do teste ANOVA:")
    print(anova_df)
    
    # Salvar resultados
    class_stats.to_csv('src/eda/results/tables/class_statistics.csv')
    anova_df.to_csv('src/eda/results/tables/anova_results.csv', index=False)
    
    return class_stats, anova_df

# Executar an√°lise por classe
class_stats, anova_results = attack_class_analysis(df)
```

### FASE 5: An√°lise de Outliers

#### Passo 5.1: Detec√ß√£o de Outliers
```python
def comprehensive_outlier_analysis(df):
    """An√°lise completa de outliers"""
    print("=" * 50)
    print("AN√ÅLISE DE OUTLIERS")
    print("=" * 50)
    
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'ts' in numeric_features:
        numeric_features.remove('ts')
    
    outlier_summary = []
    
    for feature in numeric_features:
        # M√©todo IQR
        Q1 = df[feature].quantile(0.25)
        Q3 = df[feature].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers_iqr = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
        
        # M√©todo Z-score
        z_scores = np.abs((df[feature] - df[feature].mean()) / df[feature].std())
        outliers_zscore = df[z_scores > 3]
        
        outlier_summary.append({
            'Feature': feature,
            'IQR_Outliers': len(outliers_iqr),
            'IQR_Percentage': (len(outliers_iqr) / len(df)) * 100,
            'ZScore_Outliers': len(outliers_zscore),
            'ZScore_Percentage': (len(outliers_zscore) / len(df)) * 100,
            'Lower_Bound': lower_bound,
            'Upper_Bound': upper_bound
        })
    
    outlier_df = pd.DataFrame(outlier_summary)
    
    # Visualiza√ß√£o de outliers
    plot_outlier_boxplots(df, numeric_features[:12])
    
    print("\nResumo de outliers:")
    print(outlier_df[['Feature', 'IQR_Percentage', 'ZScore_Percentage']])
    
    # Salvar resultados
    outlier_df.to_csv('src/eda/results/tables/outlier_analysis.csv', index=False)
    
    return outlier_df

def plot_outlier_boxplots(df, features):
    """Plota boxplots para visualizar outliers"""
    n_features = len(features)
    n_cols = 4
    n_rows = (n_features + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))
    axes = axes.flatten()
    
    for i, feature in enumerate(features):
        df[feature].plot(kind='box', ax=axes[i])
        axes[i].set_title(f'{feature}')
        axes[i].set_ylabel('Valor')
    
    # Remover subplots extras
    for i in range(n_features, len(axes)):
        axes[i].remove()
    
    plt.tight_layout()
    plt.show()

# Executar an√°lise de outliers
outlier_analysis = comprehensive_outlier_analysis(df)
```

### FASE 6: An√°lise Multivariada

#### Passo 6.1: An√°lise de Componentes Principais (PCA)
```python
def pca_analysis(df, n_components=10):
    """An√°lise de Componentes Principais"""
    print("=" * 50)
    print("AN√ÅLISE DE COMPONENTES PRINCIPAIS")
    print("=" * 50)
    
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    
    # Preparar dados
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'ts' in numeric_features:
        numeric_features.remove('ts')
    
    X = df[numeric_features].fillna(0)
    
    # Normalizar dados
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Aplicar PCA
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    # An√°lise de vari√¢ncia explicada
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)
    
    # Visualiza√ß√µes
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Vari√¢ncia explicada por componente
    axes[0,0].bar(range(1, n_components+1), explained_variance, alpha=0.7)
    axes[0,0].set_title('Vari√¢ncia Explicada por Componente')
    axes[0,0].set_xlabel('Componente Principal')
    axes[0,0].set_ylabel('Vari√¢ncia Explicada')
    
    # Vari√¢ncia explicada acumulada
    axes[0,1].plot(range(1, n_components+1), cumulative_variance, 'ro-')
    axes[0,1].set_title('Vari√¢ncia Explicada Acumulada')
    axes[0,1].set_xlabel('Componente Principal')
    axes[0,1].set_ylabel('Vari√¢ncia Explicada Acumulada')
    
    # Proje√ß√£o 2D das duas primeiras componentes
    scatter = axes[1,0].scatter(X_pca[:, 0], X_pca[:, 1], 
                               c=df['label'].astype('category').cat.codes, 
                               alpha=0.6, cmap='tab10')
    axes[1,0].set_title('Proje√ß√£o PCA - PC1 vs PC2')
    axes[1,0].set_xlabel('PC1')
    axes[1,0].set_ylabel('PC2')
    
    # Heatmap dos loadings
    loadings = pca.components_[:5]  # Primeiras 5 componentes
    sns.heatmap(loadings, xticklabels=numeric_features, 
                yticklabels=[f'PC{i+1}' for i in range(5)], 
                cmap='coolwarm', center=0, ax=axes[1,1])
    axes[1,1].set_title('Loadings das Componentes Principais')
    
    plt.tight_layout()
    plt.show()
    
    # An√°lise dos loadings
    loadings_df = pd.DataFrame(
        pca.components_[:5].T,
        columns=[f'PC{i+1}' for i in range(5)],
        index=numeric_features
    )
    
    print(f"\nVari√¢ncia explicada pelas primeiras {n_components} componentes:")
    for i, var in enumerate(explained_variance):
        print(f"PC{i+1}: {var:.3f} ({var*100:.1f}%)")
    
    print(f"\nVari√¢ncia total explicada: {cumulative_variance[-1]:.3f} ({cumulative_variance[-1]*100:.1f}%)")
    
    # Salvar resultados
    pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)]).to_csv(
        'src/eda/results/tables/pca_components.csv', index=False
    )
    loadings_df.to_csv('src/eda/results/tables/pca_loadings.csv')
    
    return pca, X_pca, loadings_df

# Executar PCA
pca_results, pca_components, pca_loadings = pca_analysis(df, n_components=15)
```

#### Passo 6.2: An√°lise de Import√¢ncia de Features
```python
def feature_importance_analysis(df, label_col='label'):
    """An√°lise de import√¢ncia de features usando Random Forest"""
    print("=" * 50)
    print("AN√ÅLISE DE IMPORT√ÇNCIA DE FEATURES")
    print("=" * 50)
    
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import LabelEncoder
    
    # Preparar dados
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'ts' in numeric_features:
        numeric_features.remove('ts')
    
    X = df[numeric_features].fillna(0)
    y = df[label_col]
    
    # Codificar labels se necess√°rio
    if y.dtype == 'object':
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
    else:
        y_encoded = y
    
    # Treinar Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y_encoded)
    
    # Calcular import√¢ncia
    feature_importance = pd.DataFrame({
        'Feature': numeric_features,
        'Importance': rf.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    # Visualiza√ß√µes
    fig, axes = plt.subplots(2, 2, figsize=(20, 12))
    
    # Top 20 features mais importantes
    top_20 = feature_importance.head(20)
    sns.barplot(data=top_20, x='Importance', y='Feature', ax=axes[0,0])
    axes[0,0].set_title('Top 20 Features Mais Importantes')
    axes[0,0].set_xlabel('Import√¢ncia')
    
    # Distribui√ß√£o das import√¢ncias
    axes[0,1].hist(feature_importance['Importance'], bins=30, alpha=0.7, color='skyblue')
    axes[0,1].set_title('Distribui√ß√£o das Import√¢ncias')
    axes[0,1].set_xlabel('Import√¢ncia')
    axes[0,1].set_ylabel('Frequ√™ncia')
    
    # Import√¢ncia acumulada
    cumulative_importance = np.cumsum(feature_importance['Importance'])
    axes[1,0].plot(range(len(cumulative_importance)), cumulative_importance, 'b-')
    axes[1,0].set_title('Import√¢ncia Acumulada')
    axes[1,0].set_xlabel('N√∫mero de Features')
    axes[1,0].set_ylabel('Import√¢ncia Acumulada')
    axes[1,0].grid(True)
    
    # Heatmap das correla√ß√µes das top features
    top_10_features = feature_importance.head(10)['Feature'].tolist()
    corr_top = df[top_10_features].corr()
    sns.heatmap(corr_top, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])
    axes[1,1].set_title('Correla√ß√µes - Top 10 Features')
    
    plt.tight_layout()
    plt.show()
    
    # An√°lise de redund√¢ncia
    print("\nTop 10 features mais importantes:")
    print(feature_importance.head(10))
    
    # Encontrar n√∫mero de features que explicam 90% da import√¢ncia
    cumsum_importance = np.cumsum(feature_importance['Importance'])
    n_features_90 = np.argmax(cumsum_importance >= 0.9) + 1
    
    print(f"\nN√∫mero de features que explicam 90% da import√¢ncia: {n_features_90}")
    
    # Salvar resultados
    feature_importance.to_csv('src/eda/results/tables/feature_importance.csv', index=False)
    
    return feature_importance, rf

# Executar an√°lise de import√¢ncia
feature_importance, rf_model = feature_importance_analysis(df)
```

### FASE 7: An√°lise Temporal

#### Passo 7.1: An√°lise de Padr√µes Temporais
```python
def temporal_analysis(df):
    """An√°lise de padr√µes temporais nos dados"""
    print("=" * 50)
    print("AN√ÅLISE TEMPORAL")
    print("=" * 50)
    
    # Converter timestamp
    df['datetime'] = pd.to_datetime(df['ts'], unit='s')
    df['hour'] = df['datetime'].dt.hour
    df['day_of_week'] = df['datetime'].dt.dayofweek
    df['date'] = df['datetime'].dt.date
    
    # An√°lise por hora
    hourly_analysis = df.groupby(['hour', 'label']).size().unstack(fill_value=0)
    
    # An√°lise por dia da semana
    daily_analysis = df.groupby(['day_of_week', 'label']).size().unstack(fill_value=0)
    
    # An√°lise por data
    date_analysis = df.groupby(['date', 'label']).size().unstack(fill_value=0)
    
    # Visualiza√ß√µes
    fig, axes = plt.subplots(2, 2, figsize=(20, 12))
    
    # Distribui√ß√£o por hora
    hourly_analysis.plot(kind='bar', stacked=True, ax=axes[0,0])
    axes[0,0].set_title('Distribui√ß√£o de Ataques por Hora')
    axes[0,0].set_xlabel('Hora do Dia')
    axes[0,0].set_ylabel('N√∫mero de Ataques')
    axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # Heatmap por hora
    hourly_heatmap = hourly_analysis.T
    sns.heatmap(hourly_heatmap, cmap='YlOrRd', ax=axes[0,1])
    axes[0,1].set_title('Heatmap de Ataques por Hora')
    axes[0,1].set_xlabel('Hora do Dia')
    axes[0,1].set_ylabel('Tipo de Ataque')
    
    # Distribui√ß√£o por dia da semana
    daily_analysis.plot(kind='bar', stacked=True, ax=axes[1,0])
    axes[1,0].set_title('Distribui√ß√£o de Ataques por Dia da Semana')
    axes[1,0].set_xlabel('Dia da Semana (0=Segunda)')
    axes[1,0].set_ylabel('N√∫mero de Ataques')
    axes[1,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # S√©rie temporal
    if len(date_analysis) > 1:
        date_analysis.sum(axis=1).plot(kind='line', ax=axes[1,1])
        axes[1,1].set_title('S√©rie Temporal - Total de Ataques')
        axes[1,1].set_xlabel('Data')
        axes[1,1].set_ylabel('N√∫mero de Ataques')
        axes[1,1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # Salvar resultados
    hourly_analysis.to_csv('src/eda/results/tables/hourly_analysis.csv')
    daily_analysis.to_csv('src/eda/results/tables/daily_analysis.csv')
    
    return hourly_analysis, daily_analysis, date_analysis

# Executar an√°lise temporal
hourly_data, daily_data, date_data = temporal_analysis(df)
```

### FASE 8: Gera√ß√£o de Relat√≥rio Final

#### Passo 8.1: Relat√≥rio Automatizado
```python
def generate_comprehensive_report(df, analysis_results):
    """Gera relat√≥rio completo da EDA"""
    print("=" * 50)
    print("GERANDO RELAT√ìRIO FINAL")
    print("=" * 50)
    
    report = {
        'dataset_overview': {
            'total_records': len(df),
            'total_features': len(df.columns),
            'memory_usage_mb': df.memory_usage().sum() / 1024**2,
            'date_range': f"{df['datetime'].min()} to {df['datetime'].max()}",
            'attack_types': df['label'].nunique(),
            'unique_attacks': df['label'].unique().tolist()
        },
        'data_quality': {
            'missing_values': df.isnull().sum().sum(),
            'duplicate_records': df.duplicated().sum(),
            'missing_percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        },
        'class_distribution': {
            'most_common_attack': df['label'].value_counts().index[0],
            'least_common_attack': df['label'].value_counts().index[-1],
            'imbalance_ratio': df['label'].value_counts().max() / df['label'].value_counts().min()
        },
        'feature_analysis': {
            'numeric_features': len(df.select_dtypes(include=[np.number]).columns),
            'categorical_features': len(df.select_dtypes(include=['object']).columns),
            'high_correlation_pairs': len(analysis_results.get('high_correlations', [])),
            'top_important_feature': analysis_results.get('feature_importance', pd.DataFrame()).iloc[0]['Feature'] if 'feature_importance' in analysis_results else 'N/A'
        },
        'outlier_analysis': {
            'features_with_outliers': len(analysis_results.get('outlier_analysis', [])),
            'avg_outlier_percentage': analysis_results.get('outlier_analysis', pd.DataFrame())['IQR_Percentage'].mean() if 'outlier_analysis' in analysis_results else 0
        },
        'temporal_patterns': {
            'peak_hour': hourly_data.sum(axis=1).idxmax() if 'hourly_data' in locals() else 'N/A',
            'peak_day': daily_data.sum(axis=1).idxmax() if 'daily_data' in locals() else 'N/A'
        },
        'recommendations': []
    }
    
    # Gerar recomenda√ß√µes
    if report['data_quality']['missing_percentage'] > 5:
        report['recommendations'].append("Implementar estrat√©gia robusta de tratamento de valores ausentes")
    
    if report['class_distribution']['imbalance_ratio'] > 10:
        report['recommendations'].append("Aplicar t√©cnicas de balanceamento de classes (SMOTE, undersampling)")
    
    if report['feature_analysis']['high_correlation_pairs'] > 10:
        report['recommendations'].append("Considerar remo√ß√£o de features altamente correlacionadas")
    
    if report['outlier_analysis']['avg_outlier_percentage'] > 10:
        report['recommendations'].append("Implementar tratamento de outliers")
    
    # Salvar relat√≥rio
    import json
    with open('src/eda/results/reports/eda_report.json', 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    # Imprimir resumo
    print("RESUMO EXECUTIVO:")
    print(f"- Total de registros: {report['dataset_overview']['total_records']:,}")
    print(f"- Tipos de ataques: {report['dataset_overview']['attack_types']}")
    print(f"- Qualidade dos dados: {100 - report['data_quality']['missing_percentage']:.1f}%")
    print(f"- Raz√£o de desbalanceamento: {report['class_distribution']['imbalance_ratio']:.1f}:1")
    print(f"- Features num√©ricas: {report['feature_analysis']['numeric_features']}")
    
    if report['recommendations']:
        print("\nRECOMENDA√á√ïES:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"{i}. {rec}")
    
    return report

# Compilar resultados das an√°lises
analysis_results = {
    'high_correlations': high_corr,
    'feature_importance': feature_importance,
    'outlier_analysis': outlier_analysis
}

# Gerar relat√≥rio final
final_report = generate_comprehensive_report(df, analysis_results)
```

---

## üîß Utilidades e Fun√ß√µes Auxiliares

### Fun√ß√£o para Criar Estrutura de Pastas
```python
def create_eda_structure():
    """Cria estrutura de pastas para o projeto EDA"""
    import os
    
    directories = [
        'src/eda/utils',
        'src/eda/results/figures',
        'src/eda/results/tables',
        'src/eda/results/reports'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"Criada pasta: {directory}")

# Executar no in√≠cio do projeto
create_eda_structure()
```

### Fun√ß√£o de Monitoramento de Mem√≥ria
```python
def monitor_memory_usage():
    """Monitora uso de mem√≥ria durante a an√°lise"""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    print(f"Uso de mem√≥ria: {memory_info.rss / 1024**2:.2f} MB")
    print(f"Uso de mem√≥ria virtual: {memory_info.vms / 1024**2:.2f} MB")
    
    return memory_info

# Usar periodicamente durante a an√°lise
monitor_memory_usage()
```

### Fun√ß√£o de Backup dos Resultados
```python
def backup_results():
    """Faz backup dos resultados da EDA"""
    import shutil
    import datetime
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = f"src/eda/results_backup_{timestamp}"
    
    if os.path.exists('src/eda/results'):
        shutil.copytree('src/eda/results', backup_dir)
        print(f"Backup criado em: {backup_dir}")

# Executar ao final da an√°lise
backup_results()
```

---

## üìä Checklist de Valida√ß√£o

### ‚úÖ Valida√ß√£o da An√°lise
- [ ] Todos os arquivos CSV foram carregados corretamente
- [ ] Distribui√ß√£o de classes analisada e documentada
- [ ] Estat√≠sticas descritivas calculadas para todas as features
- [ ] Matriz de correla√ß√£o gerada e interpretada
- [ ] Outliers identificados e quantificados
- [ ] An√°lise PCA realizada com interpreta√ß√£o dos componentes
- [ ] Import√¢ncia das features calculada
- [ ] Padr√µes temporais analisados
- [ ] Relat√≥rio final gerado

### ‚úÖ Valida√ß√£o dos Resultados
- [ ] Gr√°ficos salvos em alta resolu√ß√£o
- [ ] Tabelas exportadas em formato CSV
- [ ] Relat√≥rio JSON gerado com m√©tricas principais
- [ ] Recomenda√ß√µes para pr√©-processamento documentadas
- [ ] Backup dos resultados realizado

### ‚úÖ Documenta√ß√£o
- [ ] C√≥digo comentado e documentado
- [ ] Metodologia explicada
- [ ] Limita√ß√µes identificadas
- [ ] Pr√≥ximos passos definidos

---

## üéØ Pr√≥ximos Passos

### Fase de Pr√©-processamento
1. **Limpeza de Dados:** Implementar tratamento de valores ausentes e outliers
2. **Normaliza√ß√£o:** Aplicar StandardScaler ou MinMaxScaler
3. **Balanceamento:** Implementar SMOTE ou outras t√©cnicas
4. **Sele√ß√£o de Features:** Usar resultados da an√°lise de import√¢ncia

### Fase de Modelagem
1. **Baseline Models:** Implementar modelos simples (Logistic Regression, Random Forest)
2. **Advanced Models:** Testar XGBoost, LightGBM, Neural Networks
3. **Ensemble Methods:** Combinar m√∫ltiplos modelos
4. **Hyperparameter Tuning:** Otimizar par√¢metros dos modelos

### Fase de Valida√ß√£o
1. **Cross-Validation:** Implementar valida√ß√£o cruzada temporal
2. **Metrics Evaluation:** Calcular m√©tricas espec√≠ficas para IDS
3. **Confusion Matrix Analysis:** Analisar tipos de erros
4. **Feature Importance Validation:** Verificar estabilidade das features importantes

---

**Nota:** Este framework deve ser adaptado conforme as especificidades do ambiente de desenvolvimento e recursos computacionais dispon√≠veis.

**Estimativa de Tempo:** 20-30 horas para implementa√ß√£o completa  
**Recursos Necess√°rios:** 8-16 GB RAM, processador multi-core  
**Depend√™ncias:** Python 3.8+, bibliotecas cient√≠ficas padr√£o 