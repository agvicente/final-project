# ğŸ“Š Guia Completo de Preprocessing - SeparaÃ§Ã£o Treino/Teste Correta

## ğŸ¯ **VisÃ£o Geral**

Este preprocessing implementa a **abordagem CORRETA** para evitar data leakage:
1. âœ… **Separa** treino/teste **ANTES** da normalizaÃ§Ã£o
2. âœ… **Normaliza** usando parÃ¢metros aprendidos **APENAS** do treino
3. âœ… **Aplica** esses parÃ¢metros ao teste (simula produÃ§Ã£o)

## ğŸ“ **Arquivos Gerados**

O preprocessing gera os seguintes arquivos em `data/processed/`:

### ğŸ“‹ **1. Dados CSV (LegÃ­veis)**
```
train_normalized.csv    # Dados de treino normalizados + labels
test_normalized.csv     # Dados de teste normalizados + labels
```

### ğŸ”§ **2. Objetos Python**
```
scaler.pkl             # StandardScaler treinado (CRUCIAL!)
```

### âš¡ **3. Arrays NumPy (ML Otimizado)**
```
X_train.npy            # Features de treino (matriz NumPy)
X_test.npy             # Features de teste (matriz NumPy)
y_train.npy            # Labels de treino (array NumPy)
y_test.npy             # Labels de teste (array NumPy)
```

### ğŸ“Š **4. Metadados**
```
preprocessing_metadata.json  # InformaÃ§Ãµes sobre o processo
```

---

## ğŸš€ **Como Usar nos Modelos de ML**

### **MÃ©todo 1: Carregar Arrays NumPy (Recomendado para ML)**
```python
import numpy as np
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Carregar dados prÃ©-processados
X_train = np.load('data/processed/X_train.npy')
X_test = np.load('data/processed/X_test.npy')
y_train = np.load('data/processed/y_train.npy')
y_test = np.load('data/processed/y_test.npy')

print(f"Treino: {X_train.shape}, Teste: {X_test.shape}")

# Treinar modelo
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Avaliar
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

### **MÃ©todo 2: Carregar CSV (Para anÃ¡lise)**
```python
import pandas as pd

# Carregar dados
train_df = pd.read_csv('data/processed/train_normalized.csv')
test_df = pd.read_csv('data/processed/test_normalized.csv')

# Separar features e labels
X_train = train_df.drop('Label', axis=1)
y_train = train_df['Label']
X_test = test_df.drop('Label', axis=1)
y_test = test_df['Label']

# Usar normalmente...
```

### **MÃ©todo 3: Usar Scaler em Novos Dados**
```python
import joblib
import pandas as pd

# Carregar scaler treinado
scaler = joblib.load('data/processed/scaler.pkl')

# Aplicar em novos dados (produÃ§Ã£o)
new_data = pd.read_csv('new_iot_data.csv')
new_data_clean = handle_missing_values(new_data)

# Remover coluna Label se existir
features = new_data_clean.drop('Label', axis=1, errors='ignore')

# Normalizar usando MESMO scaler do treino
new_data_normalized = scaler.transform(features)

# Fazer prediÃ§Ãµes
predictions = model.predict(new_data_normalized)
```

---

## âš™ï¸ **ConfiguraÃ§Ã£o (80/20 ConfigurÃ¡vel)**

Edite `configs/preprocessing.yaml`:

```yaml
input_file: data/processed/sampled.csv
output_dir: data/processed
test_size: 0.2        # 20% para teste (configurÃ¡vel)
random_state: 42      # Para reprodutibilidade
stratify: true        # Manter proporÃ§Ã£o de classes
```

### **Exemplos de ConfiguraÃ§Ã£o:**
```yaml
# 70/30 split
test_size: 0.3

# 90/10 split  
test_size: 0.1

# Sem stratify (nÃ£o recomendado para datasets desbalanceados)
stratify: false
```

---

## ğŸ” **VerificaÃ§Ã£o de Data Leakage**

### **âœ… Abordagem CORRETA (implementada):**
```python
# 1. Separar primeiro
X_train, X_test, y_train, y_test = train_test_split(X, y)

# 2. Normalizar depois (scaler vÃª sÃ³ treino)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit_transform
X_test_scaled = scaler.transform(X_test)        # transform apenas

# âœ… Resultado: sem data leakage
```

### **âŒ Abordagem INCORRETA (evitada):**
```python
# 1. Normalizar primeiro (âŒ ERRADO)
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)  # VÃª TUDO!

# 2. Separar depois
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y)

# âŒ Resultado: data leakage!
```

---

## ğŸ“ˆ **VerificaÃ§Ã£o das EstatÃ­sticas**

### **Esperado apÃ³s normalizaÃ§Ã£o:**
```
Treino - MÃ©dia: ~0.000000, Std: ~1.000000  âœ…
Teste  - MÃ©dia: ~0.023456, Std: ~0.987654  âœ… (NORMAL!)
```

### **âš ï¸ IMPORTANTE:**
- Treino tem mÃ©dia â‰ˆ 0 e std â‰ˆ 1 (esperado)
- Teste pode ter valores ligeiramente diferentes (NORMAL!)
- Isso simula o cenÃ¡rio real de produÃ§Ã£o

---

## ğŸ› ï¸ **Pipeline Completo de ML**

```python
import numpy as np
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import json

# 1. Carregar dados prÃ©-processados
X_train = np.load('data/processed/X_train.npy')
X_test = np.load('data/processed/X_test.npy')
y_train = np.load('data/processed/y_train.npy')
y_test = np.load('data/processed/y_test.npy')

# 2. Carregar metadados
with open('data/processed/preprocessing_metadata.json', 'r') as f:
    metadata = json.load(f)

print(f"Features: {len(metadata['feature_columns'])}")
print(f"Config usado: {metadata['config']}")

# 3. Treinar mÃºltiplos modelos
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42),
    'SVM': SVC(random_state=42)
}

results = {}
for name, model in models.items():
    print(f"\nğŸ”„ Treinando {name}...")
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    results[name] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'report': classification_report(y_test, y_pred, output_dict=True)
    }
    
    print(f"AcurÃ¡cia: {results[name]['accuracy']:.4f}")

# 4. Salvar melhor modelo
best_model = max(results.items(), key=lambda x: x[1]['accuracy'])
print(f"\nğŸ† Melhor modelo: {best_model[0]}")

# Retreinar e salvar
final_model = models[best_model[0]]
final_model.fit(X_train, y_train)
joblib.dump(final_model, 'data/models/best_model.pkl')
```

---

## ğŸ”„ **Para Novos Dados em ProduÃ§Ã£o**

```python
# FunÃ§Ã£o para processar novos dados
def predict_new_data(new_csv_path):
    # 1. Carregar scaler e modelo treinados
    scaler = joblib.load('data/processed/scaler.pkl')
    model = joblib.load('data/models/best_model.pkl')
    
    # 2. Carregar e limpar novos dados
    new_data = pd.read_csv(new_csv_path)
    new_data_clean = handle_missing_values(new_data)
    
    # 3. Remover coluna label se existir
    features = new_data_clean.drop('Label', axis=1, errors='ignore')
    
    # 4. Normalizar usando scaler treinado
    features_normalized = scaler.transform(features)
    
    # 5. Fazer prediÃ§Ãµes
    predictions = model.predict(features_normalized)
    probabilities = model.predict_proba(features_normalized)
    
    return predictions, probabilities

# Usar funÃ§Ã£o
preds, probs = predict_new_data('novos_dados.csv')
```

---

## ğŸ“Š **Resumo dos BenefÃ­cios**

1. **âœ… Sem Data Leakage**: Scaler nunca vÃª dados de teste
2. **âœ… ReprodutÃ­vel**: Random state fixo
3. **âœ… ConfigurÃ¡vel**: Split 80/20 ajustÃ¡vel
4. **âœ… Completo**: CSV + NumPy + Pickle + Metadados
5. **âœ… ProduÃ§Ã£o**: Scaler reutilizÃ¡vel para novos dados
6. **âœ… Eficiente**: Arrays NumPy para ML
7. **âœ… AuditÃ¡vel**: Metadados completos
