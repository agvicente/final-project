# AcurÃ¡cia (Accuracy)

> **Tipo:** MÃ©trica de AvaliaÃ§Ã£o  
> **Complexidade:** â­â˜†â˜†â˜†â˜† (BÃ¡sico)  
> **AplicaÃ§Ã£o:** ClassificaÃ§Ã£o em Machine Learning

---

## ğŸ¯ DefiniÃ§Ã£o

**AcurÃ¡cia** Ã© a mÃ©trica mais bÃ¡sica e intuitiva para avaliar o desempenho de um modelo de classificaÃ§Ã£o.

**Pergunta que responde:**
> "De todas as previsÃµes que o modelo fez, quantas estavam corretas?"

---

## ğŸ“ FÃ³rmula MatemÃ¡tica

### DefiniÃ§Ã£o Simples
```
AcurÃ¡cia = NÃºmero de PrevisÃµes Corretas / NÃºmero Total de PrevisÃµes
```

### DefiniÃ§Ã£o Formal (ClassificaÃ§Ã£o BinÃ¡ria)

```
AcurÃ¡cia = (TP + TN) / (TP + TN + FP + FN)
```

Onde:
- **TP (True Positives):** Verdadeiros Positivos - acertou positivo
- **TN (True Negatives):** Verdadeiros Negativos - acertou negativo
- **FP (False Positives):** Falsos Positivos - errou, previu positivo
- **FN (False Negatives):** Falsos Negativos - errou, previu negativo

### Matriz de ConfusÃ£o

```
                    Predito
                    +       -
           +     [ TP  |  FN ]
Real       
           -     [ FP  |  TN ]

AcurÃ¡cia = (TP + TN) / Total
```

---

## ğŸ’¡ Exemplo PrÃ¡tico - [[AplicaÃ§Ã£o_ao_IoT_IDS|Sistema IDS para IoT]]

### CenÃ¡rio: Detector de IntrusÃ£o

Seu modelo analisa **100 conexÃµes de rede IoT**:

**Dados reais:**
- 90 conexÃµes normais (legÃ­timas)
- 10 ataques (maliciosos)

**PrediÃ§Ãµes do modelo:**
- 85 conexÃµes normais identificadas corretamente (TN)
- 5 conexÃµes normais marcadas como ataque (FP - falsos alarmes)
- 8 ataques detectados corretamente (TP)
- 2 ataques que passaram despercebidos (FN - perigoso!)

**CÃ¡lculo:**
```
AcurÃ¡cia = (TP + TN) / Total
         = (8 + 85) / 100
         = 93 / 100
         = 0.93 = 93%
```

**InterpretaÃ§Ã£o:** O modelo acertou 93% das classificaÃ§Ãµes.

---

## âš ï¸ LimitaÃ§Ãµes CrÃ­ticas

### Problema 1: Datasets Desbalanceados

**CenÃ¡rio problemÃ¡tico:**

Dataset com 95 conexÃµes normais e 5 ataques.

**Modelo "preguiÃ§oso"** que sempre prevÃª "normal":

```
PrediÃ§Ãµes: [Normal, Normal, Normal, ..., Normal]
           (100 vezes)

Resultados:
- TP = 0 (nenhum ataque detectado!)
- TN = 95 (todas as normais corretas)
- FP = 0
- FN = 5 (todos os ataques perdidos!)

AcurÃ¡cia = (0 + 95) / 100 = 95% âœ“ Parece excelente!
```

**MAS:** O modelo **NÃƒO DETECTA NENHUM ATAQUE!** Ã‰ completamente inÃºtil para IDS! ğŸš¨

### Problema 2: Custos Desiguais de Erros

Em [[AplicaÃ§Ã£o_ao_IoT_IDS|sistemas IDS]]:
- **FN (ataque nÃ£o detectado):** ğŸ’€ Muito perigoso! Sistema comprometido!
- **FP (falso alarme):** ğŸ˜ Irritante, mas nÃ£o crÃ­tico

A acurÃ¡cia **trata ambos os erros igualmente**, nÃ£o refletindo o impacto real.

### Problema 3: [[Intervalos_de_ConfianÃ§a|Intervalos de ConfianÃ§a]] Inadequados

Reportar apenas "93% de acurÃ¡cia" nÃ£o diz nada sobre:
- QuÃ£o confiÃ¡vel Ã© essa estimativa?
- Qual a margem de erro?
- O modelo Ã© realmente melhor que outro com 91%?

Veja [[The_Balanced_Accuracy_and_Its_Posterior_Distribution#Problema 1|problema detalhado no artigo]].

---

## âœ… Quando Usar AcurÃ¡cia?

AcurÃ¡cia Ã© apropriada quando:

1. **âœ… Classes balanceadas:** Aproximadamente mesmo nÃºmero de exemplos de cada classe
2. **âœ… Custos iguais:** Errar em qualquer direÃ§Ã£o tem o mesmo impacto
3. **âœ… Contexto geral:** VocÃª quer uma visÃ£o geral simplificada

**Exemplo adequado:** Classificar gatos vs. cachorros em dataset balanceado.

---

## ğŸš« Quando NÃƒO Usar AcurÃ¡cia?

Evite acurÃ¡cia quando:

1. **âŒ Classes desbalanceadas:** Uma classe muito mais frequente que outra
2. **âŒ Custos assimÃ©tricos:** Um tipo de erro Ã© mais grave
3. **âŒ Contextos crÃ­ticos:** SaÃºde, seguranÃ§a, finanÃ§as

**Exemplo inadequado:** DetecÃ§Ã£o de cÃ¢ncer (poucos casos positivos, FN Ã© fatal).

---

## ğŸ”„ Alternativas e Complementos

### Para Datasets Desbalanceados

Use [[AcurÃ¡cia_Balanceada]]:
```
Balanced Accuracy = Â½(TP/P + TN/N)
```

**Vantagem:** NÃ£o Ã© enganada por desbalanceamento!

### Para Foco em Uma Classe

**Precision (PrecisÃ£o):**
```
Precision = TP / (TP + FP)
```
*"Das previsÃµes positivas, quantas estavam corretas?"*

**Recall/Sensitivity (Sensibilidade):**
```
Recall = TP / (TP + FN)
```
*"Dos positivos reais, quantos foram detectados?"*

**F1-Score (MÃ©dia HarmÃ´nica):**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

### Para AnÃ¡lise Completa

- **ROC-AUC:** Curva ROC e Ã¡rea sob a curva
- **Precision-Recall Curve:** Especialmente para datasets desbalanceados
- **Cohen's Kappa:** AcurÃ¡cia ajustada por concordÃ¢ncia ao acaso

---

## ğŸ“Š Modelagem ProbabilÃ­stica da AcurÃ¡cia

### Abordagem Bayesiana (do [[The_Balanced_Accuracy_and_Its_Posterior_Distribution|artigo principal]])

Ao invÃ©s de reportar apenas um ponto (ex: 93%), modelar a **distribuiÃ§Ã£o completa** da acurÃ¡cia.

**Framework:**
```
Dados: C corretos, I incorretos
Prior: Beta(1, 1) - uniforme
Posterior: A ~ Beta(C+1, I+1)
```

Veja [[DistribuiÃ§Ã£o_Beta]] para detalhes.

**Exemplo:**
```python
from scipy import stats

# 93 corretos, 7 incorretos
C, I = 93, 7
posterior = stats.beta(C+1, I+1)

# EstatÃ­sticas
print(f"MÃ©dia: {posterior.mean():.3f}")
print(f"Mediana: {posterior.median():.3f}")
print(f"IC 95%: {posterior.interval(0.95)}")

# Resultado:
# MÃ©dia: 0.931
# Mediana: 0.932
# IC 95%: (0.873, 0.971)
```

Veja [[InferÃªncia_Bayesiana]] para o paradigma completo.

---

## ğŸ§® ImplementaÃ§Ã£o PrÃ¡tica

### Python (NumPy)

```python
import numpy as np

def accuracy(y_true, y_pred):
    """Calcula acurÃ¡cia."""
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return correct / total

# Exemplo IoT-IDS
y_true = np.array([0,0,0,1,1,0,1,0,0,1])  # 0=normal, 1=ataque
y_pred = np.array([0,0,1,1,1,0,1,0,0,0])  # prediÃ§Ãµes

acc = accuracy(y_true, y_pred)
print(f"AcurÃ¡cia: {acc:.2%}")  # 70%
```

### Python (Scikit-learn)

```python
from sklearn.metrics import accuracy_score, confusion_matrix

# Dados
y_true = [0,0,0,1,1,0,1,0,0,1]
y_pred = [0,0,1,1,1,0,1,0,0,0]

# AcurÃ¡cia
acc = accuracy_score(y_true, y_pred)
print(f"AcurÃ¡cia: {acc:.2%}")

# Matriz de confusÃ£o
cm = confusion_matrix(y_true, y_pred)
print("Matriz de ConfusÃ£o:")
print(cm)
```

### Com Posterior Beta

```python
from scipy import stats
import numpy as np

def accuracy_with_posterior(y_true, y_pred, confidence=0.95):
    """Calcula acurÃ¡cia com intervalo de credibilidade."""
    correct = np.sum(y_true == y_pred)
    incorrect = len(y_true) - correct
    
    # Posterior Beta
    posterior = stats.beta(correct + 1, incorrect + 1)
    
    # EstatÃ­sticas
    mean = posterior.mean()
    median = posterior.median()
    mode = correct / len(y_true)  # MLE
    ci = posterior.interval(confidence)
    
    return {
        'accuracy': mean,
        'median': median,
        'mode': mode,
        'ci': ci,
        'posterior': posterior
    }

# Exemplo
y_true = np.random.randint(0, 2, 100)
y_pred = np.random.randint(0, 2, 100)

result = accuracy_with_posterior(y_true, y_pred)
print(f"AcurÃ¡cia: {result['accuracy']:.3f}")
print(f"IC 95%: [{result['ci'][0]:.3f}, {result['ci'][1]:.3f}]")
```

---

## ğŸ“š ReferÃªncias

### Livros
- **Alpaydin, E.** (2020). *Introduction to Machine Learning* (4th ed.). MIT Press. [CapÃ­tulo sobre avaliaÃ§Ã£o]
- **Bishop, C.M.** (2006). *Pattern Recognition and Machine Learning*. Springer. [SeÃ§Ã£o 1.5.4]
- **Hastie, T., Tibshirani, R., & Friedman, J.** (2009). *The Elements of Statistical Learning*. Springer. [CapÃ­tulo 7]

### Papers
- **Brodersen et al.** (2010). "The balanced accuracy and its posterior distribution". ICPR. [[The_Balanced_Accuracy_and_Its_Posterior_Distribution]]
- **Sokolova, M. & Lapalme, G.** (2009). "A systematic analysis of performance measures for classification tasks". *Information Processing & Management*, 45(4), 427-437.

### Online
- [Scikit-learn: Accuracy Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)
- [Confusion Matrix Visualization](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)

Veja [[ReferÃªncias_BibliogrÃ¡ficas]] para lista completa.

---

## ğŸ”— Conceitos Relacionados

### Fundamentos
- [[MÃ©dia_Desvio_PadrÃ£o_Erro_PadrÃ£o]] - Como agregar acurÃ¡cias de vÃ¡rios folds
- [[Intervalos_de_ConfianÃ§a]] - Incerteza sobre a estimativa

### Alternativas Superiores
- [[AcurÃ¡cia_Balanceada]] - SoluÃ§Ã£o para desbalanceamento
- [[The_Balanced_Accuracy_and_Its_Posterior_Distribution]] - Artigo principal

### Modelagem ProbabilÃ­stica
- [[DistribuiÃ§Ã£o_Beta]] - Modelo para proporÃ§Ãµes
- [[InferÃªncia_Bayesiana]] - Paradigma probabilÃ­stico

### AplicaÃ§Ã£o
- [[AplicaÃ§Ã£o_ao_IoT_IDS]] - Uso em seu projeto de pesquisa

---

## ğŸ¯ ExercÃ­cios

Veja [[ExercÃ­cios_PrÃ¡ticos#AcurÃ¡cia]] para problemas prÃ¡ticos.

---

## ğŸ“Œ Resumo Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ACURÃCIA                      â”‚
â”‚                                         â”‚
â”‚  "ProporÃ§Ã£o de acertos totais"         â”‚
â”‚                                         â”‚
â”‚  FÃ³rmula: (TP + TN) / Total            â”‚
â”‚                                         â”‚
â”‚  âœ… Vantagens:                          â”‚
â”‚     â€¢ Simples e intuitiva              â”‚
â”‚     â€¢ FÃ¡cil de explicar                â”‚
â”‚     â€¢ Boa para classes balanceadas     â”‚
â”‚                                         â”‚
â”‚  âŒ LimitaÃ§Ãµes:                         â”‚
â”‚     â€¢ Enganosa em datasets desbalanceados â”‚
â”‚     â€¢ Ignora custos de erros diferentes â”‚
â”‚     â€¢ NÃ£o quantifica incerteza         â”‚
â”‚                                         â”‚
â”‚  â¡ï¸ SoluÃ§Ã£o: Balanced Accuracy         â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Tags:** #metrics #accuracy #classification #machine-learning #evaluation #basic

**Voltar para:** [[INDEX]]  
**PrÃ³ximo conceito:** [[AcurÃ¡cia_Balanceada]]


